{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment-2 Data Science\n",
    "\n",
    "### Deep Nimesh Shah - B00796368\n",
    "### Ravi Tulsi Zala - B00805073\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries and Packages\n",
    "****\n",
    "#### Install NLTK platform and sklearn library\n",
    "* Import tokenize module from nltk for splliting the setences into words. Commonly used words such as 'a','an','the' etc are known as Stop words are removed using the corpus module of nltk. Stemming process uses the stem module from nltk. Expressing the multiple words that occur commonly can be done using collocation module of nltk.\n",
    "\n",
    "* Import the modules like feature extraction for converting corpus into bag of words and tfIdf representation. Text-classification using NaiveBayes and Support Vector Machine from sklearn modules. Confusion matrices are reported for test sets using the confusion matrix module of sklearn.\n",
    "\n",
    "* Before using nltk platform, download the required libraries using  below commands in jupyter\n",
    "    ```\n",
    "    import nltk\n",
    "    nltk.download()\n",
    "    \n",
    "    ```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from pprint import pprint\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.collocations import *\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import pandas as pd\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset \n",
    "\n",
    "* Fetch  the Data from only four news group such as alt.atheism, talk.religion.misc, comp.graphics, sci.space form the  20 newsgrup dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Documents :  3387\n"
     ]
    }
   ],
   "source": [
    "# https://scikit-learn.org/0.19/datasets/twenty_newsgroups.html\n",
    "# https://stackoverflow.com/questions/5618878/how-to-convert-list-to-string\n",
    "\n",
    "categories_newsgroup = ['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']\n",
    "\n",
    "def loadData(categories_newsgroup):\n",
    "    \n",
    "    newsgroups_All = fetch_20newsgroups(categories=categories_newsgroup,subset='all')\n",
    "    print(\"Number of Documents : \" , newsgroups_All.filenames.shape[0])\n",
    "    text_data = ' '.join(newsgroups_All.data) \n",
    "    return text_data,newsgroups_All\n",
    "\n",
    "text_data, newsgroups_All =loadData(categories_newsgroup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1 a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenized Data\n",
    "\n",
    "\n",
    "* We have tokenized the entire dataset in the list of words and display the 50 words to demonstrate from entire list of tokenized words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['From', ':', 'healta', '@', 'saturn.wwc.edu', '(', 'Tammy', 'R', 'Healy', ')', 'Subject', ':', 'Re', ':', 'who', 'are', 'we', 'to', 'judge', ',', 'Bobby', '?', 'Lines', ':', '38', 'Organization', ':', 'Walla', 'Walla', 'College', 'Lines', ':', '38', 'In', 'article', '<', '1993Apr14.213356.22176', '@', 'ultb.isc.rit.edu', '>', 'snm6394', '@', 'ultb.isc.rit.edu', '(', 'S.N', '.', 'Mozumder', ')', 'writes', ':']\n"
     ]
    }
   ],
   "source": [
    "# https://kite.com/python/docs/nltk.tokenize.word_tokenize\n",
    "tokenized_word=word_tokenize(text_data)\n",
    "print(tokenized_word[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part-Of-Speech (POS) tagging\n",
    "\n",
    "Part of speech includes noun, pronoun, verb, adverb, adjective, conjunction, preposition, and interjection. NLTK module pos_tag is useful for tagging the words according part of speech.\n",
    "\n",
    "Some of the tags for the grammar are : \n",
    "\n",
    "* VB verb, e.g. 'judge'\n",
    "* NN noun, e.g. 'Subject'\n",
    "* RB adverb, e.g 'very'\n",
    "* JJ adjective, e.g. 'small'\n",
    "* IN preposition, e.g. 'in'\n",
    "\n",
    "Results :\n",
    "* We have performed POS tagging for the tokenized words and display the 50 results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('From', 'IN'), (':', ':'), ('healta', 'NN'), ('@', 'NN'), ('saturn.wwc.edu', 'NN'), ('(', '('), ('Tammy', 'NNP'), ('R', 'NNP'), ('Healy', 'NNP'), (')', ')'), ('Subject', 'NN'), (':', ':'), ('Re', 'NN'), (':', ':'), ('who', 'WP'), ('are', 'VBP'), ('we', 'PRP'), ('to', 'TO'), ('judge', 'VB'), (',', ','), ('Bobby', 'NNP'), ('?', '.'), ('Lines', 'NNS'), (':', ':'), ('38', 'CD'), ('Organization', 'NN'), (':', ':'), ('Walla', 'NNP'), ('Walla', 'NNP'), ('College', 'NNP'), ('Lines', 'NNP'), (':', ':'), ('38', 'CD'), ('In', 'IN'), ('article', 'NN'), ('<', '$'), ('1993Apr14.213356.22176', 'CD'), ('@', 'NNP'), ('ultb.isc.rit.edu', 'JJ'), ('>', 'NNP'), ('snm6394', 'NN'), ('@', 'NNP'), ('ultb.isc.rit.edu', 'NN'), ('(', '('), ('S.N', 'NNP'), ('.', '.'), ('Mozumder', 'NNP'), (')', ')'), ('writes', 'VBZ'), (':', ':')]\n"
     ]
    }
   ],
   "source": [
    "# https://www.nltk.org/book/ch05.html\n",
    "# https://stackoverflow.com/questions/10897339/python-fetch-first-10-results-from-a-list\n",
    "pos_tag_words = nltk.pos_tag(tokenized_word)\n",
    "print(pos_tag_words[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning\n",
    "Data cleaning is required before creating the bigrams otherwise bigrams result may consist special characters and unnecessary words which reduce the significance of creating the bigram.Only 200 results are displayed after cleaning the data.\n",
    "Following Operations are performed to clean the data.\n",
    "\n",
    "1. Remove the stop words ('are','is','everything' etc)\n",
    "2. Remove the word with a single character (e.g. 'c','a' etc)\n",
    "3. Remove special character and numbers ('@','.',')','(' etc)\n",
    "4. Remove all the null values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['From', 'healta', 'saturnwwcedu', 'Tammy', 'Healy', 'Subject', 'Re', 'judge', 'Bobby', 'Lines', 'Organization', 'Walla', 'Walla', 'College', 'Lines', 'In', 'article', 'Apr', 'ultbiscritedu', 'snm', 'ultbiscritedu', 'SN', 'Mozumder', 'writes', 'From', 'snm', 'ultbiscritedu', 'SN', 'Mozumder', 'Subject', 'Re', 'judge', 'Bobby', 'Date', 'Wed', 'Apr', 'GMT', 'In', 'article', 'healta', 'saturnwwcedu', 'healta', 'saturnwwcedu', 'TAMMY', 'HEALY', 'writes', 'Bobby', 'would', 'like', 'take', 'liberty', 'quote', 'Christian', 'writer', 'named', 'Ellen', 'White', 'hope', 'said', 'help', 'edit', 'remarks', 'group', 'future', 'Do', 'set', 'standard', 'Do', 'make', 'opinions', 'views', 'duty', 'interpretations', 'scripture', 'criterion', 'others', 'heart', 'condemn', 'come', 'ideal', 'Thoughts', 'Fromthe', 'Mount', 'Blessing', 'hope', 'quoting', 'nt', 'make', 'atheists', 'gag', 'think', 'Ellen', 'White', 'put', 'better', 'could', 'Tammy', 'Point', 'Peace', 'Bobby', 'Mozumder', 'My', 'point', 'set', 'views', 'way', 'believe', 'Saying', 'eveil', 'world', 'caused', 'atheism', 'ridiculous', 'counterproductive', 'dialogue', 'newsgroups', 'see', 'posts', 'spirit', 'condemnation', 'atheists', 'newsgroup', 'bacause', 'don', 'believe', 'exactly', 'If', 're', 'try', 'convert', 'atheists', 're', 'failing', 'miserably', 'Who', 'wants', 'position', 'constantly', 'defending', 'agaist', 'insulting', 'attacks', 'like', 'seem', 'like', 'sorry', 're', 'blind', 'nt', 'get', 'messgae', 'quote', 'everyone', 'else', 'seemed', 'Tammy', 'From', 'jk', 'lehtoricctutfi', 'Kouhia', 'Juhana', 'Subject', 'Re', 'More', 'gray', 'levels', 'screen', 'Organization', 'Tampere', 'University', 'Technology', 'Lines', 'Distribution', 'inet', 'NNTPPostingHost', 'cctutfi', 'In', 'article', 'Apr', 'cisuabedu', 'sloan', 'cisuabedu', 'Kenneth', 'Sloan', 'writes', 'Why', 'nt', 'create', 'greylevel', 'images', 'display', 'time', 'slices', 'By', 'grey', 'level', 'images', 'mean', 'items', 'bit']\n"
     ]
    }
   ],
   "source": [
    "# https://stackoverflow.com/questions/43761318/python-remove-empty-element-by-list\n",
    "# https://www.geeksforgeeks.org/removing-stop-words-nltk-python/\n",
    "# https://stackoverflow.com/questions/27199837/strip-all-non-alphabetic-characters-from-beginning-of-python-string-w-o-using-re\n",
    "# https://www.geeksforgeeks.org/python-program-check-string-contains-special-character/\n",
    "\n",
    "stop_words=set(stopwords.words(\"english\"))\n",
    "\n",
    "def cleaningForBigrams(stop_words):\n",
    "    \n",
    "    filtered_sent=[]\n",
    "    Remove_null = [] \n",
    "    basic_clean = []\n",
    "    Remove_non_alphabetic = []\n",
    "    \n",
    "    pattern = re.compile('[@_!#$%^&*()<>?/\\|}{~:]')\n",
    "    \n",
    "    tokenized_sent = tokenized_word\n",
    "    for w in tokenized_sent:\n",
    "        if w not in stop_words:\n",
    "            filtered_sent.append(w)\n",
    "\n",
    "    for w in filtered_sent:\n",
    "        if ((len(re.sub(r'[^A-Za-z]', '', w)) == 1) or (len(w) == 1) or ( pattern.search(w) != None )):\n",
    "            continue\n",
    "        else:\n",
    "            Remove_non_alphabetic.append(re.sub(r'[^A-Za-z]', '', w))\n",
    "\n",
    "    null_removed = [x for x in Remove_non_alphabetic if x]\n",
    "    basic_clean_list = null_removed\n",
    "    \n",
    "    return basic_clean_list\n",
    "\n",
    "basic_clean_list = cleaningForBigrams(stop_words)\n",
    "print(basic_clean_list[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1 b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collocation \n",
    "It is considered as a phrase which consists of multiple words that are co-occur in the text.\n",
    "For ex. In the set of education related document word 'Machine learning' co-occur together rather than individual word 'Machine' and 'Learning'.\n",
    "\n",
    "### Bigrams\n",
    "Bigram is concatenation of two words which is considered as an individual words which helps to improve insight analysis of the text document while solving NLP related problems.\n",
    "\n",
    "We have used below four techniques to filter out meaningful collocations from the document.\n",
    "1. Frequency Counting\n",
    "2. Pointwise Mutual Information (PMI)\n",
    "3. T-test \n",
    "4. Chi-square\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency without filter\n",
    "\n",
    "To count the frequency for the bigrams ngram_fd.items() from the class BigramCollocationFinder is used. This method will return bigram_freq which is a doctionary.It is casted to a list and sorted using the frequency. Below function does not filter out the collocations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequency Without Filter : \n",
      " \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bigram</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(Subject, Re)</td>\n",
       "      <td>2434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>(In, article)</td>\n",
       "      <td>1954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>(Lines, In)</td>\n",
       "      <td>1015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>(article, Apr)</td>\n",
       "      <td>570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1307</th>\n",
       "      <td>(writes, In)</td>\n",
       "      <td>554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3217</th>\n",
       "      <td>(ca, nt)</td>\n",
       "      <td>505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1298</th>\n",
       "      <td>(Organization, University)</td>\n",
       "      <td>495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2053</th>\n",
       "      <td>(Lines, NNTPPostingHost)</td>\n",
       "      <td>471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>(Distribution, world)</td>\n",
       "      <td>368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>(Lines, Distribution)</td>\n",
       "      <td>316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1472</th>\n",
       "      <td>(nt, know)</td>\n",
       "      <td>288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1526</th>\n",
       "      <td>(world, NNTPPostingHost)</td>\n",
       "      <td>216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2429</th>\n",
       "      <td>(sandvik, newtonapplecom)</td>\n",
       "      <td>211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3502</th>\n",
       "      <td>(nt, think)</td>\n",
       "      <td>211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2907</th>\n",
       "      <td>(Of, course)</td>\n",
       "      <td>204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2207</th>\n",
       "      <td>(Henry, Spencer)</td>\n",
       "      <td>179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>(would, like)</td>\n",
       "      <td>178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1168</th>\n",
       "      <td>(Computer, Science)</td>\n",
       "      <td>175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4658</th>\n",
       "      <td>(XNewsreader, TIN)</td>\n",
       "      <td>164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4659</th>\n",
       "      <td>(TIN, version)</td>\n",
       "      <td>164</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          bigram  freq\n",
       "5                  (Subject, Re)  2434\n",
       "15                 (In, article)  1954\n",
       "14                   (Lines, In)  1015\n",
       "16                (article, Apr)   570\n",
       "1307                (writes, In)   554\n",
       "3217                    (ca, nt)   505\n",
       "1298  (Organization, University)   495\n",
       "2053    (Lines, NNTPPostingHost)   471\n",
       "243        (Distribution, world)   368\n",
       "160        (Lines, Distribution)   316\n",
       "1472                  (nt, know)   288\n",
       "1526    (world, NNTPPostingHost)   216\n",
       "2429   (sandvik, newtonapplecom)   211\n",
       "3502                 (nt, think)   211\n",
       "2907                (Of, course)   204\n",
       "2207            (Henry, Spencer)   179\n",
       "38                 (would, like)   178\n",
       "1168         (Computer, Science)   175\n",
       "4658          (XNewsreader, TIN)   164\n",
       "4659              (TIN, version)   164"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def freqBigramsBeforeFiltering(basic_clean_list):\n",
    "    \n",
    "    bigrams = nltk.collocations.BigramAssocMeasures()\n",
    "    bigramFinder = nltk.collocations.BigramCollocationFinder.from_words(basic_clean_list)\n",
    "    bigram_freq = bigramFinder.ngram_fd.items()\n",
    "    bigramFreqTable = pd.DataFrame(list(bigram_freq), columns=['bigram','freq']).sort_values(by='freq', ascending=False)\n",
    "    return bigrams,bigramFinder,bigramFreqTable\n",
    "    \n",
    "bigrams,bigramFinder,bigramFreqTable = freqBigramsBeforeFiltering(basic_clean_list)  \n",
    "print(\"Frequency Without Filter : \\n \\n\")\n",
    "bigramFreqTable.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pointwise Mutual Information (PMI)\n",
    "\n",
    "PMI is a measure for how a one word is associated to other word in n-grams.\n",
    "\n",
    "A pmi(x,y) = 0 means that the words  x and y are  independent, positive PMI means x and y co-occur more frequently than,and negative PMI means they cooccur less frequently. PMI is calculated as shown in the below formula :\n",
    "\n",
    "\n",
    "#### Note :-\n",
    "\n",
    "PMI is generally used after filtering out the collocations as it is more sensitive to independent words which co-occur rarely. In the below function PMI is used on unfiltered collocations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pointwise Mutual Information (PMI) : \n",
      " \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bigram_pmi</th>\n",
       "      <th>PMI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(AAO, AngloAustralian)</td>\n",
       "      <td>19.217605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1804</th>\n",
       "      <td>(expound, Ambrose)</td>\n",
       "      <td>19.217605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1796</th>\n",
       "      <td>(evolutionaire, eindprodukten)</td>\n",
       "      <td>19.217605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1797</th>\n",
       "      <td>(evolutions, hitherto)</td>\n",
       "      <td>19.217605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1798</th>\n",
       "      <td>(exasperating, Sexism)</td>\n",
       "      <td>19.217605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1799</th>\n",
       "      <td>(excepting, Pfaith)</td>\n",
       "      <td>19.217605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1800</th>\n",
       "      <td>(excessively, darken)</td>\n",
       "      <td>19.217605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1801</th>\n",
       "      <td>(exiled, Assyrian)</td>\n",
       "      <td>19.217605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1802</th>\n",
       "      <td>(experi, ences)</td>\n",
       "      <td>19.217605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1803</th>\n",
       "      <td>(expired, gunpowder)</td>\n",
       "      <td>19.217605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1805</th>\n",
       "      <td>(exter, nally)</td>\n",
       "      <td>19.217605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1857</th>\n",
       "      <td>(gf, msuinfoclmsuedu)</td>\n",
       "      <td>19.217605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1806</th>\n",
       "      <td>(extravagant, REFUSAL)</td>\n",
       "      <td>19.217605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1807</th>\n",
       "      <td>(ezzie, lucslancsacuk)</td>\n",
       "      <td>19.217605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1808</th>\n",
       "      <td>(fa, thers)</td>\n",
       "      <td>19.217605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1809</th>\n",
       "      <td>(fabulous, feather)</td>\n",
       "      <td>19.217605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1810</th>\n",
       "      <td>(facinated, pnumatic)</td>\n",
       "      <td>19.217605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1811</th>\n",
       "      <td>(factorymade, ammonium)</td>\n",
       "      <td>19.217605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1812</th>\n",
       "      <td>(farewell, Berryman)</td>\n",
       "      <td>19.217605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1813</th>\n",
       "      <td>(fatherly, admonitions)</td>\n",
       "      <td>19.217605</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          bigram_pmi        PMI\n",
       "0             (AAO, AngloAustralian)  19.217605\n",
       "1804              (expound, Ambrose)  19.217605\n",
       "1796  (evolutionaire, eindprodukten)  19.217605\n",
       "1797          (evolutions, hitherto)  19.217605\n",
       "1798          (exasperating, Sexism)  19.217605\n",
       "1799             (excepting, Pfaith)  19.217605\n",
       "1800           (excessively, darken)  19.217605\n",
       "1801              (exiled, Assyrian)  19.217605\n",
       "1802                 (experi, ences)  19.217605\n",
       "1803            (expired, gunpowder)  19.217605\n",
       "1805                  (exter, nally)  19.217605\n",
       "1857           (gf, msuinfoclmsuedu)  19.217605\n",
       "1806          (extravagant, REFUSAL)  19.217605\n",
       "1807          (ezzie, lucslancsacuk)  19.217605\n",
       "1808                     (fa, thers)  19.217605\n",
       "1809             (fabulous, feather)  19.217605\n",
       "1810           (facinated, pnumatic)  19.217605\n",
       "1811         (factorymade, ammonium)  19.217605\n",
       "1812            (farewell, Berryman)  19.217605\n",
       "1813         (fatherly, admonitions)  19.217605"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def getPMI(bigramFinder,bigrams):\n",
    "    \n",
    "    bigramPMITable_PMI = pd.DataFrame(list(bigramFinder.score_ngrams(bigrams.pmi)), columns=['bigram_pmi','PMI']).sort_values(by='PMI', ascending=False)\n",
    "    return bigramPMITable_PMI\n",
    "    \n",
    "bigramPMITable_PMI = getPMI(bigramFinder,bigrams)\n",
    "print(\"Pointwise Mutual Information (PMI) : \\n \\n\")\n",
    "bigramPMITable_PMI.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chi Square\n",
    "Chi Square test is useful for testing or rejecting null hypothesis. Chi square test works well with the categorical data. It gives the information based on how data is divided among categories but it can not suggest the meaningfulness. Null hypothesis means there is no relationship between two variables are zero or independent variables. Function below will show top 20 results of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chi Square (Hypothesis Testing) : \n",
      " \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bigram_chi</th>\n",
       "      <th>chi_sq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(AAAA, BBBB)</td>\n",
       "      <td>609643.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2631</th>\n",
       "      <td>(finn, bscno)</td>\n",
       "      <td>609643.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2633</th>\n",
       "      <td>(firstcome, firstserved)</td>\n",
       "      <td>609643.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2634</th>\n",
       "      <td>(firstest, mostest)</td>\n",
       "      <td>609643.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2635</th>\n",
       "      <td>(fischer, iesdaucdk)</td>\n",
       "      <td>609643.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2636</th>\n",
       "      <td>(fitz, cseogiedu)</td>\n",
       "      <td>609643.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2637</th>\n",
       "      <td>(flagellating, genuflecting)</td>\n",
       "      <td>609643.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2638</th>\n",
       "      <td>(flamboyant, orgies)</td>\n",
       "      <td>609643.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2639</th>\n",
       "      <td>(flatten, steamroller)</td>\n",
       "      <td>609643.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2640</th>\n",
       "      <td>(flb, flboptiplanfi)</td>\n",
       "      <td>609643.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2641</th>\n",
       "      <td>(flees, resists)</td>\n",
       "      <td>609643.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2642</th>\n",
       "      <td>(flibble, glop)</td>\n",
       "      <td>609643.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2643</th>\n",
       "      <td>(flighttested, cesium)</td>\n",
       "      <td>609643.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2644</th>\n",
       "      <td>(flirting, therapist)</td>\n",
       "      <td>609643.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2645</th>\n",
       "      <td>(flowery, brownnosers)</td>\n",
       "      <td>609643.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2646</th>\n",
       "      <td>(fluent, dartmouthEDU)</td>\n",
       "      <td>609643.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2647</th>\n",
       "      <td>(fluidmechanically, compressible)</td>\n",
       "      <td>609643.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2648</th>\n",
       "      <td>(fluxes, Theorists)</td>\n",
       "      <td>609643.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2649</th>\n",
       "      <td>(flythrough, fractalgenerated)</td>\n",
       "      <td>609643.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2650</th>\n",
       "      <td>(fnord, Episkopos)</td>\n",
       "      <td>609643.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             bigram_chi    chi_sq\n",
       "0                          (AAAA, BBBB)  609643.0\n",
       "2631                      (finn, bscno)  609643.0\n",
       "2633           (firstcome, firstserved)  609643.0\n",
       "2634                (firstest, mostest)  609643.0\n",
       "2635               (fischer, iesdaucdk)  609643.0\n",
       "2636                  (fitz, cseogiedu)  609643.0\n",
       "2637       (flagellating, genuflecting)  609643.0\n",
       "2638               (flamboyant, orgies)  609643.0\n",
       "2639             (flatten, steamroller)  609643.0\n",
       "2640               (flb, flboptiplanfi)  609643.0\n",
       "2641                   (flees, resists)  609643.0\n",
       "2642                    (flibble, glop)  609643.0\n",
       "2643             (flighttested, cesium)  609643.0\n",
       "2644              (flirting, therapist)  609643.0\n",
       "2645             (flowery, brownnosers)  609643.0\n",
       "2646             (fluent, dartmouthEDU)  609643.0\n",
       "2647  (fluidmechanically, compressible)  609643.0\n",
       "2648                (fluxes, Theorists)  609643.0\n",
       "2649     (flythrough, fractalgenerated)  609643.0\n",
       "2650                 (fnord, Episkopos)  609643.0"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def getChiSq(bigramFinder,bigrams):\n",
    "\n",
    "    bigramTtable_chisq = pd.DataFrame(list(bigramFinder.score_ngrams(bigrams.chi_sq)), columns=['bigram_chi','chi_sq']).sort_values(by='chi_sq', ascending=False)\n",
    "    return bigramTtable_chisq\n",
    "\n",
    "bigramTtable_chisq = getChiSq(bigramFinder,bigrams)\n",
    "print(\"Chi Square (Hypothesis Testing) : \\n \\n\")\n",
    "bigramTtable_chisq.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T-test without filter\n",
    "\n",
    "It is useful to check whethere two categories are statistically different from each other.Rejecting null hypothesis means there is not a different between the means of the variables. Difference between those variables is meaningful. It will show top 20 results of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ttest Without Filter (Hypothesis Testing) : \n",
      " \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bigram_ttest</th>\n",
       "      <th>t</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(Subject, Re)</td>\n",
       "      <td>49.026437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(In, article)</td>\n",
       "      <td>43.941015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(Lines, In)</td>\n",
       "      <td>31.301873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(article, Apr)</td>\n",
       "      <td>23.720673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(writes, In)</td>\n",
       "      <td>22.935838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(ca, nt)</td>\n",
       "      <td>22.298068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(Organization, University)</td>\n",
       "      <td>21.862747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(Lines, NNTPPostingHost)</td>\n",
       "      <td>21.471784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(Distribution, world)</td>\n",
       "      <td>19.134433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>(Lines, Distribution)</td>\n",
       "      <td>17.570933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>(nt, know)</td>\n",
       "      <td>16.240285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>(world, NNTPPostingHost)</td>\n",
       "      <td>14.609433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>(sandvik, newtonapplecom)</td>\n",
       "      <td>14.519644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>(Of, course)</td>\n",
       "      <td>14.269629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>(nt, think)</td>\n",
       "      <td>13.731932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>(Henry, Spencer)</td>\n",
       "      <td>13.374119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>(Computer, Science)</td>\n",
       "      <td>13.203624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>(XNewsreader, TIN)</td>\n",
       "      <td>12.801140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>(TIN, version)</td>\n",
       "      <td>12.792806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>(version, PL)</td>\n",
       "      <td>12.789863</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  bigram_ttest          t\n",
       "0                (Subject, Re)  49.026437\n",
       "1                (In, article)  43.941015\n",
       "2                  (Lines, In)  31.301873\n",
       "3               (article, Apr)  23.720673\n",
       "4                 (writes, In)  22.935838\n",
       "5                     (ca, nt)  22.298068\n",
       "6   (Organization, University)  21.862747\n",
       "7     (Lines, NNTPPostingHost)  21.471784\n",
       "8        (Distribution, world)  19.134433\n",
       "9        (Lines, Distribution)  17.570933\n",
       "10                  (nt, know)  16.240285\n",
       "11    (world, NNTPPostingHost)  14.609433\n",
       "12   (sandvik, newtonapplecom)  14.519644\n",
       "13                (Of, course)  14.269629\n",
       "14                 (nt, think)  13.731932\n",
       "15            (Henry, Spencer)  13.374119\n",
       "16         (Computer, Science)  13.203624\n",
       "17          (XNewsreader, TIN)  12.801140\n",
       "18              (TIN, version)  12.792806\n",
       "19               (version, PL)  12.789863"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def getTtestWithoutFilter(bigramFinder,bigrams):\n",
    "    \n",
    "    bigramTable_ttest = pd.DataFrame(list(bigramFinder.score_ngrams(bigrams.student_t)), columns=['bigram_ttest','t']).sort_values(by='t', ascending=False)\n",
    "    return bigramTable_ttest\n",
    "    \n",
    "bigramTable_ttest = getTtestWithoutFilter(bigramFinder,bigrams)\n",
    "print(\"Ttest Without Filter (Hypothesis Testing) : \\n \\n\")\n",
    "bigramTable_ttest.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note:- \n",
    "From the above result of Frequency bigram and T-test method, it is observed that results consists of prepopositions, articles which does not give the meaningful result.\n",
    "To avoid this issue, we have filtered the bigram with (noun,noun) and (adjective,noun)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to Filter Bigrams\n",
    "\n",
    "Filtering the tags according to the Adjectives and noun types as shown in the below function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to filter for ADJ/NN bigrams\n",
    "type_1 = ('JJ', 'JJR', 'JJS', 'NN', 'NNS', 'NNP', 'NNPS')\n",
    "type_2 = ('NN', 'NNS', 'NNP', 'NNPS')\n",
    "\n",
    "def filterBigramsUsingTypes(bigram,type_1,type_2):\n",
    "    \n",
    "    tags = nltk.pos_tag(bigram)\n",
    "    if tags[0][1] in type_1 and tags[1][1] in type_2:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency With Filter\n",
    "\n",
    "In the below function, each bigram from bigramFreqTable is checked against the tags mentioned in the above function. It will show top 20 results of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequency With Filter : \n",
      " \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bigram</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(Subject, Re)</td>\n",
       "      <td>2434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>(article, Apr)</td>\n",
       "      <td>570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1298</th>\n",
       "      <td>(Organization, University)</td>\n",
       "      <td>495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2053</th>\n",
       "      <td>(Lines, NNTPPostingHost)</td>\n",
       "      <td>471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>(Distribution, world)</td>\n",
       "      <td>368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>(Lines, Distribution)</td>\n",
       "      <td>316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1526</th>\n",
       "      <td>(world, NNTPPostingHost)</td>\n",
       "      <td>216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2429</th>\n",
       "      <td>(sandvik, newtonapplecom)</td>\n",
       "      <td>211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2207</th>\n",
       "      <td>(Henry, Spencer)</td>\n",
       "      <td>179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1168</th>\n",
       "      <td>(Computer, Science)</td>\n",
       "      <td>175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4658</th>\n",
       "      <td>(XNewsreader, TIN)</td>\n",
       "      <td>164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4659</th>\n",
       "      <td>(TIN, version)</td>\n",
       "      <td>164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4660</th>\n",
       "      <td>(version, PL)</td>\n",
       "      <td>164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2205</th>\n",
       "      <td>(henry, zootorontoedu)</td>\n",
       "      <td>162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2581</th>\n",
       "      <td>(Jon, Livesey)</td>\n",
       "      <td>158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2052</th>\n",
       "      <td>(University, Lines)</td>\n",
       "      <td>153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2579</th>\n",
       "      <td>(livesey, solntzewpdsgicom)</td>\n",
       "      <td>153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2580</th>\n",
       "      <td>(solntzewpdsgicom, Jon)</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2192</th>\n",
       "      <td>(USA, Lines)</td>\n",
       "      <td>138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4576</th>\n",
       "      <td>(Lines, NntpPostingHost)</td>\n",
       "      <td>136</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           bigram  freq\n",
       "5                   (Subject, Re)  2434\n",
       "16                 (article, Apr)   570\n",
       "1298   (Organization, University)   495\n",
       "2053     (Lines, NNTPPostingHost)   471\n",
       "243         (Distribution, world)   368\n",
       "160         (Lines, Distribution)   316\n",
       "1526     (world, NNTPPostingHost)   216\n",
       "2429    (sandvik, newtonapplecom)   211\n",
       "2207             (Henry, Spencer)   179\n",
       "1168          (Computer, Science)   175\n",
       "4658           (XNewsreader, TIN)   164\n",
       "4659               (TIN, version)   164\n",
       "4660                (version, PL)   164\n",
       "2205       (henry, zootorontoedu)   162\n",
       "2581               (Jon, Livesey)   158\n",
       "2052          (University, Lines)   153\n",
       "2579  (livesey, solntzewpdsgicom)   153\n",
       "2580      (solntzewpdsgicom, Jon)   150\n",
       "2192                 (USA, Lines)   138\n",
       "4576     (Lines, NntpPostingHost)   136"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def getFreqWithFilter(bigramFreqTable):\n",
    "    \n",
    "    \n",
    "    filtered_freq_bi = bigramFreqTable[bigramFreqTable.bigram.map(lambda x: filterBigramsUsingTypes(x,type_1,type_2))]\n",
    "    return filtered_freq_bi\n",
    "    \n",
    "filtered_freq_bi = getFreqWithFilter(bigramFreqTable)\n",
    "print(\"Frequency With Filter : \\n \\n\")\n",
    "filtered_freq_bi.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T-test with Filter\n",
    "\n",
    "In the below function, each bigram from bigramTable_ttest is checked against the tags mentioned in the filterBigramsUsingTypes() function. It will show top 20 results of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ttest With Filter : \n",
      " \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bigram_ttest</th>\n",
       "      <th>t</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(Subject, Re)</td>\n",
       "      <td>49.026437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(article, Apr)</td>\n",
       "      <td>23.720673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(Organization, University)</td>\n",
       "      <td>21.862747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(Lines, NNTPPostingHost)</td>\n",
       "      <td>21.471784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(Distribution, world)</td>\n",
       "      <td>19.134433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>(Lines, Distribution)</td>\n",
       "      <td>17.570933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>(world, NNTPPostingHost)</td>\n",
       "      <td>14.609433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>(sandvik, newtonapplecom)</td>\n",
       "      <td>14.519644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>(Henry, Spencer)</td>\n",
       "      <td>13.374119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>(Computer, Science)</td>\n",
       "      <td>13.203624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>(XNewsreader, TIN)</td>\n",
       "      <td>12.801140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>(TIN, version)</td>\n",
       "      <td>12.792806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>(version, PL)</td>\n",
       "      <td>12.789863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>(henry, zootorontoedu)</td>\n",
       "      <td>12.722120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>(Jon, Livesey)</td>\n",
       "      <td>12.565676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>(livesey, solntzewpdsgicom)</td>\n",
       "      <td>12.364615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>(solntzewpdsgicom, Jon)</td>\n",
       "      <td>12.241505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>(University, Lines)</td>\n",
       "      <td>11.651978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>(USA, Lines)</td>\n",
       "      <td>11.608579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>(Apr, GMT)</td>\n",
       "      <td>11.592073</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   bigram_ttest          t\n",
       "0                 (Subject, Re)  49.026437\n",
       "3                (article, Apr)  23.720673\n",
       "6    (Organization, University)  21.862747\n",
       "7      (Lines, NNTPPostingHost)  21.471784\n",
       "8         (Distribution, world)  19.134433\n",
       "9         (Lines, Distribution)  17.570933\n",
       "11     (world, NNTPPostingHost)  14.609433\n",
       "12    (sandvik, newtonapplecom)  14.519644\n",
       "15             (Henry, Spencer)  13.374119\n",
       "16          (Computer, Science)  13.203624\n",
       "17           (XNewsreader, TIN)  12.801140\n",
       "18               (TIN, version)  12.792806\n",
       "19                (version, PL)  12.789863\n",
       "20       (henry, zootorontoedu)  12.722120\n",
       "22               (Jon, Livesey)  12.565676\n",
       "24  (livesey, solntzewpdsgicom)  12.364615\n",
       "25      (solntzewpdsgicom, Jon)  12.241505\n",
       "27          (University, Lines)  11.651978\n",
       "28                 (USA, Lines)  11.608579\n",
       "29                   (Apr, GMT)  11.592073"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def getTtestWithFilter(bigramTable_ttest):\n",
    "    \n",
    "    filtered_bi_ttest = bigramTable_ttest[bigramTable_ttest.bigram_ttest.map(lambda x: filterBigramsUsingTypes(x,type_1,type_2))]\n",
    "    return filtered_bi_ttest\n",
    "    \n",
    "filtered_bi_ttest=getTtestWithFilter(bigramTable_ttest)\n",
    "print(\"Ttest With Filter : \\n \\n\")\n",
    "filtered_bi_ttest.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-1 c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing the Result of Different Techniques\n",
    "\n",
    "* Frequency and T-test with filter methods give similar tuples as shown in the dataframe below. Using both the methods shows the overlapping of the tuples.\n",
    "* There are other methods like Mean and Variance, Likelihood Ratio which can be combined with the below methods to a distribution of the data to find the goodness of fit. Though T-test uses the Mean values internally.\n",
    "* Chi Square and PMI also gives the results which are better than the results obtained from the T-test and Frequency without filtering.\n",
    "* Good collocation has high PMI scores because the difference between the probability of co-occurance of words and frequency of words is low. Combining the Frequency and PMI can be good for getting the best results in terms of collocation extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Frequency Filter</th>\n",
       "      <th>Ttest</th>\n",
       "      <th>Chi Square</th>\n",
       "      <th>PMI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(Subject, Re)</td>\n",
       "      <td>(Subject, Re)</td>\n",
       "      <td>(AAAA, BBBB)</td>\n",
       "      <td>(AAO, AngloAustralian)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(article, Apr)</td>\n",
       "      <td>(article, Apr)</td>\n",
       "      <td>(finn, bscno)</td>\n",
       "      <td>(expound, Ambrose)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(Organization, University)</td>\n",
       "      <td>(Organization, University)</td>\n",
       "      <td>(firstcome, firstserved)</td>\n",
       "      <td>(evolutionaire, eindprodukten)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(Lines, NNTPPostingHost)</td>\n",
       "      <td>(Lines, NNTPPostingHost)</td>\n",
       "      <td>(firstest, mostest)</td>\n",
       "      <td>(evolutions, hitherto)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(Distribution, world)</td>\n",
       "      <td>(Distribution, world)</td>\n",
       "      <td>(fischer, iesdaucdk)</td>\n",
       "      <td>(exasperating, Sexism)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(Lines, Distribution)</td>\n",
       "      <td>(Lines, Distribution)</td>\n",
       "      <td>(fitz, cseogiedu)</td>\n",
       "      <td>(excepting, Pfaith)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(world, NNTPPostingHost)</td>\n",
       "      <td>(world, NNTPPostingHost)</td>\n",
       "      <td>(flagellating, genuflecting)</td>\n",
       "      <td>(excessively, darken)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(sandvik, newtonapplecom)</td>\n",
       "      <td>(sandvik, newtonapplecom)</td>\n",
       "      <td>(flamboyant, orgies)</td>\n",
       "      <td>(exiled, Assyrian)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(Henry, Spencer)</td>\n",
       "      <td>(Henry, Spencer)</td>\n",
       "      <td>(flatten, steamroller)</td>\n",
       "      <td>(experi, ences)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>(Computer, Science)</td>\n",
       "      <td>(Computer, Science)</td>\n",
       "      <td>(flb, flboptiplanfi)</td>\n",
       "      <td>(expired, gunpowder)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>(XNewsreader, TIN)</td>\n",
       "      <td>(XNewsreader, TIN)</td>\n",
       "      <td>(flees, resists)</td>\n",
       "      <td>(exter, nally)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>(TIN, version)</td>\n",
       "      <td>(TIN, version)</td>\n",
       "      <td>(flibble, glop)</td>\n",
       "      <td>(gf, msuinfoclmsuedu)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>(version, PL)</td>\n",
       "      <td>(version, PL)</td>\n",
       "      <td>(flighttested, cesium)</td>\n",
       "      <td>(extravagant, REFUSAL)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>(henry, zootorontoedu)</td>\n",
       "      <td>(henry, zootorontoedu)</td>\n",
       "      <td>(flirting, therapist)</td>\n",
       "      <td>(ezzie, lucslancsacuk)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>(Jon, Livesey)</td>\n",
       "      <td>(Jon, Livesey)</td>\n",
       "      <td>(flowery, brownnosers)</td>\n",
       "      <td>(fa, thers)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>(University, Lines)</td>\n",
       "      <td>(livesey, solntzewpdsgicom)</td>\n",
       "      <td>(fluent, dartmouthEDU)</td>\n",
       "      <td>(fabulous, feather)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>(livesey, solntzewpdsgicom)</td>\n",
       "      <td>(solntzewpdsgicom, Jon)</td>\n",
       "      <td>(fluidmechanically, compressible)</td>\n",
       "      <td>(facinated, pnumatic)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>(solntzewpdsgicom, Jon)</td>\n",
       "      <td>(University, Lines)</td>\n",
       "      <td>(fluxes, Theorists)</td>\n",
       "      <td>(factorymade, ammonium)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>(USA, Lines)</td>\n",
       "      <td>(USA, Lines)</td>\n",
       "      <td>(flythrough, fractalgenerated)</td>\n",
       "      <td>(farewell, Berryman)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>(Lines, NntpPostingHost)</td>\n",
       "      <td>(Apr, GMT)</td>\n",
       "      <td>(fnord, Episkopos)</td>\n",
       "      <td>(fatherly, admonitions)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Frequency Filter                        Ttest  \\\n",
       "0                 (Subject, Re)                (Subject, Re)   \n",
       "1                (article, Apr)               (article, Apr)   \n",
       "2    (Organization, University)   (Organization, University)   \n",
       "3      (Lines, NNTPPostingHost)     (Lines, NNTPPostingHost)   \n",
       "4         (Distribution, world)        (Distribution, world)   \n",
       "5         (Lines, Distribution)        (Lines, Distribution)   \n",
       "6      (world, NNTPPostingHost)     (world, NNTPPostingHost)   \n",
       "7     (sandvik, newtonapplecom)    (sandvik, newtonapplecom)   \n",
       "8              (Henry, Spencer)             (Henry, Spencer)   \n",
       "9           (Computer, Science)          (Computer, Science)   \n",
       "10           (XNewsreader, TIN)           (XNewsreader, TIN)   \n",
       "11               (TIN, version)               (TIN, version)   \n",
       "12                (version, PL)                (version, PL)   \n",
       "13       (henry, zootorontoedu)       (henry, zootorontoedu)   \n",
       "14               (Jon, Livesey)               (Jon, Livesey)   \n",
       "15          (University, Lines)  (livesey, solntzewpdsgicom)   \n",
       "16  (livesey, solntzewpdsgicom)      (solntzewpdsgicom, Jon)   \n",
       "17      (solntzewpdsgicom, Jon)          (University, Lines)   \n",
       "18                 (USA, Lines)                 (USA, Lines)   \n",
       "19     (Lines, NntpPostingHost)                   (Apr, GMT)   \n",
       "\n",
       "                           Chi Square                             PMI  \n",
       "0                        (AAAA, BBBB)          (AAO, AngloAustralian)  \n",
       "1                       (finn, bscno)              (expound, Ambrose)  \n",
       "2            (firstcome, firstserved)  (evolutionaire, eindprodukten)  \n",
       "3                 (firstest, mostest)          (evolutions, hitherto)  \n",
       "4                (fischer, iesdaucdk)          (exasperating, Sexism)  \n",
       "5                   (fitz, cseogiedu)             (excepting, Pfaith)  \n",
       "6        (flagellating, genuflecting)           (excessively, darken)  \n",
       "7                (flamboyant, orgies)              (exiled, Assyrian)  \n",
       "8              (flatten, steamroller)                 (experi, ences)  \n",
       "9                (flb, flboptiplanfi)            (expired, gunpowder)  \n",
       "10                   (flees, resists)                  (exter, nally)  \n",
       "11                    (flibble, glop)           (gf, msuinfoclmsuedu)  \n",
       "12             (flighttested, cesium)          (extravagant, REFUSAL)  \n",
       "13              (flirting, therapist)          (ezzie, lucslancsacuk)  \n",
       "14             (flowery, brownnosers)                     (fa, thers)  \n",
       "15             (fluent, dartmouthEDU)             (fabulous, feather)  \n",
       "16  (fluidmechanically, compressible)           (facinated, pnumatic)  \n",
       "17                (fluxes, Theorists)         (factorymade, ammonium)  \n",
       "18     (flythrough, fractalgenerated)            (farewell, Berryman)  \n",
       "19                 (fnord, Episkopos)         (fatherly, admonitions)  "
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def convertDfToSeries(df,resultsToShow,columnName):\n",
    "    \n",
    "    resultSeries = pd.Series()\n",
    "    series = df.head(resultsToShow)[columnName]\n",
    "    return list(series)\n",
    "    \n",
    "\n",
    "freqList = convertDfToSeries(df=filtered_freq_bi,resultsToShow=20,columnName='bigram') \n",
    "ttestList = convertDfToSeries(df=filtered_bi_ttest,resultsToShow=20,columnName='bigram_ttest')   \n",
    "chiList= convertDfToSeries(df=bigramTtable_chisq,resultsToShow=20,columnName='bigram_chi')   \n",
    "pmiList = convertDfToSeries(df=bigramPMITable_PMI,resultsToShow=20,columnName='bigram_pmi')   \n",
    "\n",
    "# https://thispointer.com/python-pandas-how-to-convert-lists-to-a-dataframe/\n",
    "\n",
    "zippedList =list(zip(freqList,ttestList,chiList,pmiList))\n",
    "\n",
    "comparisonDf = pd.DataFrame(zippedList, columns = ['Frequency Filter','Ttest','Chi Square', 'PMI'])\n",
    "comparisonDf                  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-2 a)\n",
    "### Clean the text:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Removal of Stop-words:\n",
    "Stop words such as \"a\" \"and\" \"but,\" \"how\" \"or\" and \"what\" are removed from the database as a part of the cleaning process to reduce the dimensional space while performing text mining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[':', 'healta', '@', 'saturn.wwc.edu', '(', 'Tammy', 'R', 'Healy', ')', 'Subject', ':', ':', 'judge', ',', 'Bobby', '?', 'Lines', ':', '38', 'Organization', ':', 'Walla', 'Walla', 'College', 'Lines', ':', '38', 'article', '<', '1993Apr14.213356.22176', '@', 'ultb.isc.rit.edu', '>', 'snm6394', '@', 'ultb.isc.rit.edu', '(', 'S.N', '.', 'Mozumder', ')', 'writes', ':', '>', ':', 'snm6394', '@', 'ultb.isc.rit.edu', '(', 'S.N', '.', 'Mozumder', ')', '>', 'Subject', ':', ':', 'judge', ',', 'Bobby', '?', '>', 'Date', ':', 'Wed', ',', '14', 'Apr', '1993', '21:33:56', 'GMT', '>', 'article', '<', 'healta.56.734556346', '@', 'saturn.wwc.edu', '>', 'healta', '@', 'saturn.wwc.edu', '(', 'TAMMY', 'R', 'HEALY', ')', 'writes', ':', '>', '>', 'Bobby', ',', '>', '>', '>', '>', 'would', 'like', 'take', 'liberty', 'quote', 'Christian', 'writer', 'named', '>', '>', 'Ellen', 'G.', 'White', '.', 'hope', 'said', 'help', 'edit', '>', '>', 'remarks', 'group', 'future', '.', '>', '>', '>', '>', \"''\", 'set', 'standard', '.', 'make', 'opinions', ',', 'views', '>', '>', 'duty', ',', 'interpretations', 'scripture', ',', 'criterion', 'others', '>', '>', 'heart', 'condemn', 'come', 'ideal', '.', \"''\", '>', '>', 'Thoughts', 'Fromthe', 'Mount', 'Blessing', 'p.', '124', '>', '>', '>', '>', 'hope', 'quoting', \"n't\", 'make', 'atheists', 'gag', ',', 'think', 'Ellen', 'White', '>', '>', 'put', 'better', 'could', '.', '>', '>', '>', '>', 'Tammy', '>', '>', 'Point', '?', '>', '>', 'Peace', ',', '>', '>', 'Bobby', 'Mozumder', '>', 'point', 'set', 'views', 'way', 'believe', '.', 'Saying', 'eveil', 'world', 'caused', 'atheism', 'ridiculous', 'counterproductive', 'dialogue', 'newsgroups', '.', 'see', 'posts', 'spirit', 'condemnation', 'atheists', 'newsgroup', 'bacause', \"don'\", 'believe', 'exactly', '.', \"'re\", 'try', 'convert', 'atheists', ',', \"'re\", 'failing', 'miserably', '.', 'wants', 'position', 'constantly', 'defending', 'agaist', 'insulting', 'attacks', ',', 'like', 'seem', 'like', '?', '!', \"'m\", 'sorry', \"'re\", 'blind', \"n't\", 'get', 'messgae', 'quote', ',', 'everyone', 'else', 'seemed', '.', 'Tammy']\n"
     ]
    }
   ],
   "source": [
    "#tokenized_text_clean = word_tokenize(newsgroups_All) \n",
    "stop_words=list(stopwords.words(\"english\"))\n",
    "\n",
    "def removeStopWords(stop_words):\n",
    "    \n",
    "    tokenized_word = []\n",
    "    list1=[]\n",
    "    list2=[]\n",
    "    Remove_stopword=[]\n",
    "    \n",
    "    for i in newsgroups_All.data:\n",
    "        tokenized_word.append(word_tokenize(i))\n",
    "   \n",
    "    for i in range(len(tokenized_word)):\n",
    "        for j in range(len(tokenized_word[i])):\n",
    "            #print(tokenized_word[i][j])\n",
    "            if tokenized_word[i][j].lower() not in stop_words:\n",
    "                list1.append(tokenized_word[i][j])\n",
    "        list2.append(list1)\n",
    "        list1=[]\n",
    "    \n",
    "    return list2\n",
    "    \n",
    "listAfterRemovingStopWords = removeStopWords(stop_words)\n",
    "print(listAfterRemovingStopWords[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Remove Numbers and all Non-characters\n",
    "To remove all non-characters such as digits, special character(@,!,&) we have replace them with empty string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', 'healta', '', 'saturnwwcedu', '', 'Tammy', 'R', 'Healy', '', 'Subject', '', '', 'judge', '', 'Bobby', '', 'Lines', '', '', 'Organization', '', 'Walla', 'Walla', 'College', 'Lines', '', '', 'article', '', 'Apr', '', 'ultbiscritedu', '', 'snm', '', 'ultbiscritedu', '', 'SN', '', 'Mozumder', '', 'writes', '', '', '', 'snm', '', 'ultbiscritedu', '', 'SN', '', 'Mozumder', '', '', 'Subject', '', '', 'judge', '', 'Bobby', '', '', 'Date', '', 'Wed', '', '', 'Apr', '', '', 'GMT', '', 'article', '', 'healta', '', 'saturnwwcedu', '', 'healta', '', 'saturnwwcedu', '', 'TAMMY', 'R', 'HEALY', '', 'writes', '', '', '', 'Bobby', '', '', '', '', '', 'would', 'like', 'take', 'liberty', 'quote', 'Christian', 'writer', 'named', '', '', 'Ellen', 'G', 'White', '', 'hope', 'said', 'help', 'edit', '', '', 'remarks', 'group', 'future', '', '', '', '', '', '', 'set', 'standard', '', 'make', 'opinions', '', 'views', '', '', 'duty', '', 'interpretations', 'scripture', '', 'criterion', 'others', '', '', 'heart', 'condemn', 'come', 'ideal', '', '', '', '', 'Thoughts', 'Fromthe', 'Mount', 'Blessing', 'p', '', '', '', '', '', 'hope', 'quoting', 'nt', 'make', 'atheists', 'gag', '', 'think', 'Ellen', 'White', '', '', 'put', 'better', 'could', '', '', '', '', '', 'Tammy', '', '', 'Point', '', '', '', 'Peace', '', '', '', 'Bobby', 'Mozumder', '', 'point', 'set', 'views', 'way', 'believe', '', 'Saying', 'eveil', 'world', 'caused', 'atheism', 'ridiculous', 'counterproductive', 'dialogue', 'newsgroups', '', 'see', 'posts', 'spirit', 'condemnation', 'atheists', 'newsgroup', 'bacause', 'don', 'believe', 'exactly', '', 're', 'try', 'convert', 'atheists', '', 're', 'failing', 'miserably', '', 'wants', 'position', 'constantly', 'defending', 'agaist', 'insulting', 'attacks', '', 'like', 'seem', 'like', '', '', 'm', 'sorry', 're', 'blind', 'nt', 'get', 'messgae', 'quote', '', 'everyone', 'else', 'seemed', '', 'Tammy']\n"
     ]
    }
   ],
   "source": [
    "def removeNonCharacters(listAfterRemovingStopWords):\n",
    "    \n",
    "    Remove_noncharacter=[]\n",
    "    list_after_removing_nonchars=[]\n",
    "    for i in range(len(listAfterRemovingStopWords)):\n",
    "        for j in range(len(listAfterRemovingStopWords[i])):\n",
    "            Remove_noncharacter.append(re.sub(r'[^A-Za-z]', '', listAfterRemovingStopWords[i][j]))\n",
    "        list_after_removing_nonchars.append(Remove_noncharacter)\n",
    "        Remove_noncharacter=[]\n",
    "        \n",
    "    return list_after_removing_nonchars\n",
    "    \n",
    "list_after_removing_nonchars= removeNonCharacters(listAfterRemovingStopWords)\n",
    "print(list_after_removing_nonchars[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Null Values\n",
    "We have removed all the null values from the dataset as a part of the cleaning and display the sample result.\n",
    "In the result, it can be clearly seen that there are no null values present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['healta', 'saturnwwcedu', 'Tammy', 'R', 'Healy', 'Subject', 'judge', 'Bobby', 'Lines', 'Organization', 'Walla', 'Walla', 'College', 'Lines', 'article', 'Apr', 'ultbiscritedu', 'snm', 'ultbiscritedu', 'SN', 'Mozumder', 'writes', 'snm', 'ultbiscritedu', 'SN', 'Mozumder', 'Subject', 'judge', 'Bobby', 'Date', 'Wed', 'Apr', 'GMT', 'article', 'healta', 'saturnwwcedu', 'healta', 'saturnwwcedu', 'TAMMY', 'R', 'HEALY', 'writes', 'Bobby', 'would', 'like', 'take', 'liberty', 'quote', 'Christian', 'writer', 'named', 'Ellen', 'G', 'White', 'hope', 'said', 'help', 'edit', 'remarks', 'group', 'future', 'set', 'standard', 'make', 'opinions', 'views', 'duty', 'interpretations', 'scripture', 'criterion', 'others', 'heart', 'condemn', 'come', 'ideal', 'Thoughts', 'Fromthe', 'Mount', 'Blessing', 'p', 'hope', 'quoting', 'nt', 'make', 'atheists', 'gag', 'think', 'Ellen', 'White', 'put', 'better', 'could', 'Tammy', 'Point', 'Peace', 'Bobby', 'Mozumder', 'point', 'set', 'views', 'way', 'believe', 'Saying', 'eveil', 'world', 'caused', 'atheism', 'ridiculous', 'counterproductive', 'dialogue', 'newsgroups', 'see', 'posts', 'spirit', 'condemnation', 'atheists', 'newsgroup', 'bacause', 'don', 'believe', 'exactly', 're', 'try', 'convert', 'atheists', 're', 'failing', 'miserably', 'wants', 'position', 'constantly', 'defending', 'agaist', 'insulting', 'attacks', 'like', 'seem', 'like', 'm', 'sorry', 're', 'blind', 'nt', 'get', 'messgae', 'quote', 'everyone', 'else', 'seemed', 'Tammy']\n"
     ]
    }
   ],
   "source": [
    "def removeNulls(list_after_removing_nonchars):\n",
    "    \n",
    "    list_after_removing_nulls=[]\n",
    "    for i in range(len(list_after_removing_nonchars)):\n",
    "        list_after_removing_nulls.append([x for x in list_after_removing_nonchars[i] if x])\n",
    "    \n",
    "    return list_after_removing_nulls\n",
    "    \n",
    "list_after_removing_nulls = removeNulls(list_after_removing_nonchars)\n",
    "print(list_after_removing_nulls[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stem the words\n",
    "Stemming is the process of reducing a word to its roots of words known as a lemma.\n",
    "Ex. stemming for root word like include:\n",
    "1. likes 2. likely 3. liking 4. liked\n",
    "\n",
    "We have used Potter’s Stemmer algorithm which produces best output as compared to other stemmers and it has less error rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['healta', 'saturnwwcedu', 'tammi', 'R', 'heali', 'subject', 'judg', 'bobbi', 'line', 'organ', 'walla', 'walla', 'colleg', 'line', 'articl', 'apr', 'ultbiscritedu', 'snm', 'ultbiscritedu', 'SN', 'mozumd', 'write', 'snm', 'ultbiscritedu', 'SN', 'mozumd', 'subject', 'judg', 'bobbi', 'date', 'wed', 'apr', 'gmt', 'articl', 'healta', 'saturnwwcedu', 'healta', 'saturnwwcedu', 'tammi', 'R', 'heali', 'write', 'bobbi', 'would', 'like', 'take', 'liberti', 'quot', 'christian', 'writer', 'name', 'ellen', 'G', 'white', 'hope', 'said', 'help', 'edit', 'remark', 'group', 'futur', 'set', 'standard', 'make', 'opinion', 'view', 'duti', 'interpret', 'scriptur', 'criterion', 'other', 'heart', 'condemn', 'come', 'ideal', 'thought', 'fromth', 'mount', 'bless', 'p', 'hope', 'quot', 'nt', 'make', 'atheist', 'gag', 'think', 'ellen', 'white', 'put', 'better', 'could', 'tammi', 'point', 'peac', 'bobbi', 'mozumd', 'point', 'set', 'view', 'way', 'believ', 'say', 'eveil', 'world', 'caus', 'atheism', 'ridicul', 'counterproduct', 'dialogu', 'newsgroup', 'see', 'post', 'spirit', 'condemn', 'atheist', 'newsgroup', 'bacaus', 'don', 'believ', 'exactli', 're', 'tri', 'convert', 'atheist', 're', 'fail', 'miser', 'want', 'posit', 'constantli', 'defend', 'agaist', 'insult', 'attack', 'like', 'seem', 'like', 'm', 'sorri', 're', 'blind', 'nt', 'get', 'messga', 'quot', 'everyon', 'els', 'seem', 'tammi']\n"
     ]
    }
   ],
   "source": [
    "# https://www.geeksforgeeks.org/introduction-to-stemming/\n",
    "# https://www.tutorialspoint.com/python/python_stemming_algorithms\n",
    "def stemmingProcess(list_after_removing_nulls):\n",
    "    \n",
    "    stemmed_words=[]\n",
    "    temp_stemmed_words=[]\n",
    "    \n",
    "    for i in range(len(list_after_removing_nulls)):\n",
    "        for j in range(len(list_after_removing_nulls[i])):\n",
    "            temp_stemmed_words.append(PorterStemmer().stem(list_after_removing_nulls[i][j]))\n",
    "        stemmed_words.append(temp_stemmed_words)\n",
    "        temp_stemmed_words=[]\n",
    "        \n",
    "    return stemmed_words\n",
    "\n",
    "stemmed_words = stemmingProcess(list_after_removing_nulls)\n",
    "print(stemmed_words[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data=[]\n",
    "\n",
    "for i in range(len(stemmed_words)):\n",
    "    clean_data.append(' '.join(stemmed_words[i]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-2 b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bag-of-words \n",
    "In order to perform text mining,first we need to turn the text content into numerical feature vectors. We cannot feed Raw data or a sequence of words directly to the algorithms or model as most of the model expects numerical feature vectors. scikit-learn provides utilities to extract numerical features from text content, i.e:\n",
    "\n",
    "1. **Tokenizing**:- Giving an integer id for each possible token, \n",
    "2. **Counting**:-  The occurrences of tokens in every document.\n",
    "2. **Normalizing**:- Giving weights to the token which occur majority in the documnet.\n",
    "\n",
    "tokenization, counting and normalization is also known the Bag of Words\n",
    "Using CountVectorizer we can implement tokenization and occurrence counting in a single class.\n",
    "\n",
    " CountVectorizer **class fit_transform()** method converts the data into term-document sparse matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3387, 31715)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://scikit-learn.org/0.19/datasets/twenty_newsgroups.html\n",
    "#https://scikit-learn.org/stable/modules/feature_extraction.html\n",
    "def convertToBagOfWords(clean_data):\n",
    "    \n",
    "    count_vect = CountVectorizer()\n",
    "    X_train_counts = count_vect.fit_transform(clean_data)\n",
    "    return X_train_counts\n",
    "\n",
    "X_train_counts = convertToBagOfWords(clean_data)\n",
    "\n",
    "X_train_counts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**tf stands  Term Frequency**. \n",
    "It is calculated by dividing  total number of times a term occurs in the document by the total number of terms present in the document.\n",
    "\n",
    "**idf stands for Inverse Document Frequency**. \n",
    "It is calculated by taking the logarithmic of the total number of documents in the divided by the number of documents in which the term has present. This normalization is to up-weigh the rare terms in the corpus.\n",
    "\n",
    "**tf-idf** helps us to rank the importance of a term to the document in its contextual document corpus.\n",
    "\n",
    "For example , let assume that the search query is \"what is birthplace of John snow\". During searching for documents to this query, we need to give the least importance to words \"what\",\"is\",\"of\" and give more importance to \"John\",\"snow\" and \"birthplace\".Due to low idf value of \"what\" , \"is\" , \"of\" , their contribution will be very low to the calculation of relevance of a document to the query . Documents having high occurance of \"John\",\"Snow\" and \"birthplace\" will get more weight and hence better rank ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3387, 31715)"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://www.quora.com/What-is-a-tf-idf-vector\n",
    "# https://scikit-learn.org/stable/modules/feature_extraction.html\n",
    "def convertToTfIdf(X_train_counts):\n",
    "    \n",
    "    tfidf_transformer = TfidfTransformer()\n",
    "    X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "    return X_train_tfidf\n",
    "    \n",
    "X_train_tfidf = convertToTfIdf(X_train_counts)\n",
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-2 c)\n",
    "### Text Classification\n",
    "\n",
    "Text classification is performed on data represented using the Bag Of Wods and TF-IDF vector representation. Data is split randomly using the train_test_split()  of sklearn module with 70% Trainset Size and 30% Testset size. Prefix X shows the data except the target column in both the sets whereas Prefix y shows the target column data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test,y_train,y_test = train_test_split(X_train_tfidf,newsgroups_All.target,test_size = 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split arrays or matrices into train and test subsets\n",
    "Text classification is performed on data represented using the Bag Of Wods and TF-IDF vector representation. Data is split randomly using the train_test_split()  of sklearn module with 70% Trainset Size and 30% Testset size. Prefix X shows the data except the target column in both the sets whereas Prefix y shows the target column data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2370, 31715)\n",
      "(1017, 31715)\n",
      "2370\n",
      "1017\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape[0])\n",
    "print(y_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have built our model using **Multinomial Naive Bayes** and **Support Vector Machine** classifier to calculate training and testing score.\n",
    "From the below experiment, we can say that test score of SVM is higher which is **95.08%** while test score for Naive Bayes is **92.62%**.\n",
    "\n",
    "**Parameters selection for SVM**\n",
    "\n",
    "1. **degree** :- Default value for this parameter is 3. It is the degree of the polynomial kernal function and ignored by all other kernel.\n",
    "2. **C** :- This parameter tells the SVM optimization to how much we want to avoid misclassifying each training example\n",
    "3. **Gamma** :- Gamma parameter defines how far the influence of a single training example reaches, with low values meaning far and  high values means the close.\n",
    "4. **kernel**:- It defines type of the kernel used in the algorithm. It can be linear, poly, rbf or sigmoid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t \t Accuracy of Multinomial Naive Bayes Classifier  on training set: 95.73839662447257\n",
      "\t \t Accuracy of Multinomial Naive Bayes Classifier  on Test set: 92.62536873156341 \n",
      "\n",
      "\t \t Accuracy of Support Vector Machine Classifier  on training set: 99.53586497890295\n",
      "\t \t Accuracy of Support Vector Machine Classifier  on Test set: 95.08357915437561 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n",
    "# https://www.quora.com/What-are-C-and-gamma-with-regards-to-a-support-vector-machine\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "# https://medium.com/@bedigunjit/simple-guide-to-text-classification-nlp-using-svm-and-naive-bayes-with-python-421db3a72d34\n",
    "def classifyDatasets(classifier,classifierName):\n",
    "    \n",
    "    classifierTrainset = classifier.fit(X_train, y_train)\n",
    "    accTrain = classifierTrainset.score(X_train, y_train) * 100\n",
    "    print('\\t \\t Accuracy of '+ classifierName +' on training set: ' + str(accTrain))\n",
    "    \n",
    "    accTest = classifierTrainset.score(X_test, y_test) * 100\n",
    "    print('\\t \\t Accuracy of '+ classifierName +' on Test set: ' + str(accTest),'\\n')\n",
    "    \n",
    "    y_train_pred = classifierTrainset.predict(X_train)\n",
    "    cm_train=pd.DataFrame(confusion_matrix(y_train, y_train_pred),index=newsgroups_All.target_names,columns=newsgroups_All.target_names)\n",
    "    \n",
    "    y_test_pred = classifierTrainset.predict(X_test)\n",
    "    cm_test=pd.DataFrame(confusion_matrix(y_test, y_test_pred),index=newsgroups_All.target_names,columns=newsgroups_All.target_names)\n",
    "\n",
    "    return cm_train,cm_test,accTrain,accTest\n",
    "\n",
    "multinomialNBClf = MultinomialNB()\n",
    "cm_trainNB,cm_testNB,accTrainNB,accTestNB = classifyDatasets(classifier=multinomialNBClf,\n",
    "                                                         classifierName=\"Multinomial Naive Bayes Classifier \")\n",
    "\n",
    "svmClf = svm.SVC(C=1.0, kernel='linear')\n",
    "cm_trainSVM,cm_test_SVM,accTrainSVM,accTestSVM = classifyDatasets(classifier=svmClf,\n",
    "                                                         classifierName=\"Support Vector Machine Classifier \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix:-\n",
    "Confusion matrix is a table which is used to tell us about performance of the classifier on the test data for which\n",
    "the true values are known to us.\n",
    "It helps us to determine confusion between the classes. Most commonly performance are measured with the help of confusion matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix for Multinomial Naive Bayes\n",
    "Majority confusion is observed between alt.atheism and talk.religion.misc.\n",
    "From the below confusion matrix of NB we can say that 54 documents whose actual value is classification is talk.religion.misc are classified as alt.atheism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alt.atheism</th>\n",
       "      <th>comp.graphics</th>\n",
       "      <th>sci.space</th>\n",
       "      <th>talk.religion.misc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>alt.atheism</th>\n",
       "      <td>248</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>comp.graphics</th>\n",
       "      <td>0</td>\n",
       "      <td>292</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sci.space</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>284</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>talk.religion.misc</th>\n",
       "      <td>54</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>118</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    alt.atheism  comp.graphics  sci.space  talk.religion.misc\n",
       "alt.atheism                 248              2          1                   2\n",
       "comp.graphics                 0            292          4                   0\n",
       "sci.space                     0              3        284                   0\n",
       "talk.religion.misc           54              0          9                 118"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm_testNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix for SVM\n",
    "As accuracy of SVM is higher, it is observed that there is very less confusion among the different classifier as compared to the Multinomial Naive Byes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alt.atheism</th>\n",
       "      <th>comp.graphics</th>\n",
       "      <th>sci.space</th>\n",
       "      <th>talk.religion.misc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>alt.atheism</th>\n",
       "      <td>235</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>comp.graphics</th>\n",
       "      <td>0</td>\n",
       "      <td>295</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sci.space</th>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>278</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>talk.religion.misc</th>\n",
       "      <td>13</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>159</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    alt.atheism  comp.graphics  sci.space  talk.religion.misc\n",
       "alt.atheism                 235              3          0                  15\n",
       "comp.graphics                 0            295          1                   0\n",
       "sci.space                     1              7        278                   1\n",
       "talk.religion.misc           13              8          1                 159"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm_test_SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Comparison of score between Multinomial Naive Byes and SVM\n",
    "We have plotted the bar chart which shows the accuracy (score) of Multinomial Naive Bayes and Support Vector Machine (SVM).\n",
    "From the graph we can easily conclude that SVM has a higher accuracy.\n",
    "\n",
    "**justification**:- Support Vector Machine (SVM) gives better result with the full-length content whereas Multinomial Naive Bayes (MNB) is better at snippets and short documents.\n",
    "In our dataset there are large documents which makes SVM as ideal candidate in terms of accuracy. When there are more training cases SVM provides good accuracy as compared to NVB.\n",
    "The advantage of SVMs is its robustness. Moreover, SVMs do not require any hyperparameter for tuning as it can append good parameters by its own. SVMs generalize way better than MNB which removes the need for the feature selection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEZCAYAAABmTgnDAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzt3Xm8HfP9x/HXW2JNLCGhCEJtRS0VShuaFm0polVbKVqt6q8b1Za22qZ0CS2lexWlqH0tWmoJpbbEvtYWEoLQRIQi5PP74/s9Mjk5587c3HNyT3Pfz8fjPu7Md+bM9zNzZs5n1u8oIjAzM+vKQr0dgJmZdT4nCzMzK+VkYWZmpZwszMyslJOFmZmVcrIwM7NSThZmFUiaIWmN3o6jRtIXJT2X41puHj4/VtLn2hTbqjmufrl/BUk3SHpZ0rGSviPppHbUbe3T55JF3kimSlq0t2NpF0mjJN0labqkFyRdI2lYB8S1tqTzckwvSbpH0tdrPyqdLCIGRsTjvR0HgKSFgeOAD+e4XmwwziKSRkt6RNIrkiZIOmV+rAcR8VSO661cdCDwArBURBwaET+JiLYkqnaQdH9OfjMkvSXptUL/d3ow3bMlHVEyzifzdjJd0hRJ/5A0tMK015X05rzG1kifShZ5Q9kKCGDn+Vx3//lUz5rAn4FDgaWB1YHfArNaWIckdWvdkfRO4FZgIvDuiFga2A0YDizZqthabX59b920ArAYcH8X45xPWsc/RVoPNgLGA9u0Pbq5rQY8ED18Anhe1rtWiIj1c/IbCPwT+HKtPyJ+0q56Ja0HnAR8mfQdvhP4Iy3clrslIvrMH/B94CbSXtlldcMWB44FngReAm4EFs/DRgD/AqaRfuz2z+Vjgc8VprE/cGOhP4AvAY8AT+SyE/I0ppM23q0K4/cDvgM8Brych68C/AY4ti7evwIHN5jHTwJ3dbEMGtaRh70PuD3P/+3A+wqfGwv8OC+//wJrklbgk4HJwNPAj4B+Teo9A7i85PvZmfQDOC3X967CsAnAN4F7gFdyvSsAf8vzcTUwKI87LC/7A4FncnyHFqa1OXBzrmcy8GtgkZLvLYA1c/cOwAO53qeBbxQ++3ngUeA/wKXASnXTPShPd2r+XtVkWSwKHJ/jfyZ3Lwqsnec/gBnAtQ0+u23+jlbpYlmPJa+7pB+ha4EXSUcAZwLLFMY9LM/ny8DDwDaF5TiOtC4/BxxXt/z7A6cCM4E3crzbAqOBMwrT34LZ29fdwMiS9W5/4PEczxPA3t1ZhnnYSGASaafq+bwefKbCb8jby62u/At52fwHuBxYubC9/RqYQtqu7gbWAb6al8vrebmc12Ca+wC3lGzL38vLYo7vLc9TbR2ZAWzS49/Pnk7gf+mPtBH/H7Bp/qJWKAz7TV4RVs5fwvvyyrZqXin3AhYGlgM2brTi0DhZ/ANYltmJZ588jf55RX0WWCwP+yZwb16ZRNobXI60UT4DLJTHGwy8Woy/UOcawGvAL4APAgPrhjerY1nSD9inc2x75f7lCvP6FLB+Hr4wcDHwB2AAsDxwG/CFJsv+2a42Rmb/CG6Xp/2t/H0tkodPAG4hJYiV88ZwB7BJ/p6uBX6Qxx2Wl/1ZObZ3kzbWbfPwTUk/UP3zuA9SSLxNvrdisphMTvLAIOA9uftDpI32PTmmXwE31E33MmAZ0no1Bfhok+VxZJ7f5YEhpB/To+rmr3+Tz44Bri/ZFsYyO1msmZf7ormuG4Dj87B1SDs3KxXqfmfuvhn4dO4eCGzRKD5SwvhRoe7R5GSRv8sXSQl4oRzHi8CQJuvd0qTktE4eviKw/jwsw5HAm3mchXP9r5J3OKost0LZnnkdWjtP60fAdXnYqLyclsrztz6wfB52NnBEF3WtS0omP8vxDqgbfjjpSGcl0pHmqcCfCp99s6W/n62cWCf/kY4OZgKDc/9DwCG5eyHSXstGDT73beCiKisOjZPFh0rimlqrl7RnMqrJeA8C2+XuLwNXdDHNLYBzST9Gr+WVaGBXdZCSxG11ZTcz51HUkYVhK+QVefFC2V61jaTB9GfS5IcxD/8ecG6hfyHS3uzI3D+Bwh4kcAHwu0L/V4CLc/ewvOzXLQw/Bji5Sd0HF7/jRt8bcyaLp0h7kkvVjXMycEyhf2Ce72GFaYwoDD8XOLxJTI8BOxT6PwJMqJu/Zsnij8DZJevdHOtu3bBdgDtz95qkxLwtsHDdeDcAPyRvU4XyOeKj62RxGHB63eevBPZrst4NIB2B7Fpc9+ZhGY4kbfP9C8OfJye87iw34Lq6dXPh/L2vQEpC95N2+FT3uS6TRR5nBGldfyHHexKzd2CeAN5fGHd1UsITbUgWfemaxX7AVRHxQu7/Sy6DtKe+GGnlqrdKk/KqJhZ7JB0q6cF8gXcaaU9pcIW6TiMdlZD/n96swoi4JSJ2j4ghpGs0WwPfLaljJdIpuKInSXt+jeZlNdJGMVnStDwvfyDtxTXyImkvsJk56o+IWbm+Yv3PFbr/26B/YN00i/E+meuoXWi/TNKzkqYDP2H2d9Dos/V2Jf0IPCnpeklbNpmHGaT5Ls7Ds4XuVxvEXFP/fbwdfwVly3oOkpbPF1ufzsvjDPLyiIhHScl0NPB8Hq8WxwGkvemHJN0uaceqdRasBuxWW4fyejSiLv63v4uIeAXYg3Q6b7KkyyWt22TaZcvwxYgoXgTu6vsom4ffF+KfQjpqGUo6TXoyadt4TtJvJVWuIyJujIhdI2Iw6cj1I8C3JIm0LV9RqPdO0k5Wt++Oq6JPJAtJiwO7Ax/IPxDPAocAG0naiJS1XyOdu603sUk5pNMmSxT639FgnCjEsRVpT2p30uHuMqTzmKpQ1xnAqBzvu0ingEpFxO3AhcAGJXU8Q1rpi1Yl7d3PNS95Oq+T9iqXyX9LRcT6TUK5mvQj28wc9Rc2hqebfqLcKoXuVXMdAL8jHVmuFRFLka7hqO6zQRMRcXtEjCIlxotJRwiN5mEAacOdl3mo/z6K8Ze5Gti8yl0z2U9J87thXh77UFgeEfGXiBiR4wng6Fz+SETsRVoORwPn53nujomkI4tlCn8DImJMYZw5vouIuDIitiMllIdIR1KN9GQZdkftOmZxHhaPiPGRHBcRmwAbkk77fq02K92pJCJuBi4BNoh0KPE06Qi4WO9ieYe4W9Ouok8kC9Jh9VvAesDG+e9dpPN9++a92FOA4yStJKmfpC3z7bVnAttK2l1Sf0nLSdo4T/cu4BOSlsh3IR1QEseSpD2OKUB/Sd8nncusOQk4StJa+c6PDZXvoY+ISaSLzqcDF0TEfxtVIGmEpM9LWj73r0u6cHxLSR1XAGtL+lSezz3y8rqsUT0RMRm4CjhW0lKSFpL0TkkfaDLvPwDeJ+lnkt6RY1tT0hmSliH94H5M0jZKt4YeSkpG/+p6kXbpe/m7WR/4DHBOLl+SdN57Rl4+X6w6QaVbUveWtHREzMzTqd0i+hfgM5I2zuvOT4BbI2LCPMR+FnCEpCGSBpNuzjijygcj4mrSNZeLJG2av88lJR0k6bMNPrIk6SLoNEkrk65r1eZ3HUkfyvPzGukI7q08bB9JQ/L2My1/5C265wxgJ0kfydvdYpJGNkt0Ss9s7JyTUu3icLM653kZdtPvcz3r5BgHSdo1d28habjSXXWvkC701+J9jnSNsSFJH5T0WUlDcv/6wMeYvS3/HhgjaZU8fHlJO+VhzwP9JK3asrls5TmtTv0D/k7d3US5fHfSaYH+pLuhjidl65dI52Nr5wa3It32OZ20F7FfLh9M+sF8mXS3xmjmvmaxZqG/H+mQdDrpIum3SOfity0MP4J0LvJlUnIYWvj8PnmaH+xiXjcg3Sn1HGlDmkDa61u4rA7S4f/4PP/jmfP8+ljmPle7NGkvfVL+zJ3Anl3Etg5wHuk0Se3OkIPJd1ABHyfdZfQScD2FC5fF5ZT7zwBGF/o/B1ydu4cx591QzwLfKoy7NWmPdAZph+HIrr63YhmwSF6fpubv8fa65XQQ6TTff0iJdmiz6VJ3Lr+uvsWAX+b1ZHLuXqxu/hpes8jjLEK6nvAo6UfqSdKOwqr13yfpouv4vDzuIiXqSXnYhqQbF14uzNNKhe/g+fy5+4FdGsVXP5/MfTfUe/P3/R/SjtTljeLM/SvmcV9i9l1z683DMhxZm8dm61iTac4RT6H8gLwMpudl/ftc/lHgvryMpuRlsUQeth7pZpNpNLjGRNqpvaKwjB+ncMchaVs+jHR33cv5u/5B4fNH5zqnkW/K6cmf8kTtf4CkrUkb6LBIe3PWgNLzNE+QEmRLH0wy66v6ymmo/3n51MzXgJOcKMxsfpsvyUKpmYHnJd1XKFtW6dH1R/L/Qblckn4p6VGlx9zfMz9i7GSS3kU6lFyRdKrMzGy+ml9HFqeSzt0VHQ5cExFrAdfkfoDtgbXy34Gkc+J9WkQ8GOkOkfdFxPTejqfTRcSEiJBPQZm1znxJFhFxA+niVdEo0rMD5P+7FMr/HMktwDKSKt8zbmZmrdebjaStEOn2SyJicu1WT9IDTMUHoiblssn1E5B0IOnogwEDBmy67rrNns0xM7NGxo8f/0KkB3i71IktatY/HAVNHjCJiBOBEwGGDx8e48aNa2dcZmYLHEn1LTc01Jt3Qz1XO72U/z+fyycx55O3Q2nPU5dmZlZRbyaLS5ndNtN+pMfYa+X75ruitgBeqp2uMjOz3jG/XshzFumJycGSJpGafhgDnCvpAFIrnrvl0a8gNdL2KKlhr8/MjxjNzKy5+ZIsIjU21shcb+2K9Ej5l9obkZmZdYef4DYzs1JOFmZmVsrJwszMSjlZmJlZKScLMzMr1YlPcJtZnWGHX97bIVgHmzDmY22vw0cWZmZWysnCzMxKOVmYmVkpJwszMyvlC9z44qF1bX5cPDTrdD6yMDOzUk4WZmZWysnCzMxKOVmYmVkpJwszMyvlZGFmZqWcLMzMrJSThZmZlXKyMDOzUk4WZmZWysnCzMxKVUoWkm5pUn5ja8MxM7NOVPXIYr1ulpuZ2QKky1ZnJZ2YOxctdNesDjzUlqjMzKyjlDVR/mKT7gDuB85ueURmZtZxukwWEfFtSNcsIuKS+ROSmZl1mkrXLCLiEklbSfqNpPMBJG0iaUR7wzMzs05Q9W6oLwCnA1OA7XLxm8BP2xSXmZl1kKp3Q30T2DYiRgOzctmD+G4oM7M+oWqyWBp4IndH/t8PmNnyiMzMrONUTRY3AYfUlR0EXN/acMzMrBOV3Tpb82XgCkmfBwZKujuX79CesMzMrJNUShYRMUnSxsAIYDVgInBjRLzZzuDMzKwzVG5IMCJmRcQNwI3A4sCQVgQg6RBJ90u6T9JZkhaTtLqkWyU9IukcSYu0oi4zM5s3XSYLST+VtFehf0/gYeBc4FFJ2/akckkrA18FhkfEBqSL5nsCRwO/iIi1gKnAAT2px8zMeqbsyGI34OZC/zHAYRGxJOmC9w9bEEN/YHFJ/YElgMnAh4Dz8/DTgF1aUI+Zmc2jsmSxfERMAJC0HunU0+/ysFOAdXtSeUQ8DfwceIqUJF4CxgPTCtdDJgEr96QeMzPrmbJkMV3S4Ny9FXBHRLyW+/tV+HyXJA0CRpFasF0JGABs32DUaFCGpAMljZM0bsqUKT0JxczMulD2Y38hcKakA4HDmLOV2c2Y/aDevNoWeCIipkTEzFzf+4Bl8mkpgKHAM40+HBEnRsTwiBg+ZEhLrrebmVkDZcnim8AdpGsXZwK/LQzbHDi5h/U/BWwhaQlJArYBHgCuAz6Zx9kPcIu3Zma9qKyJ8teBbzcZdlxPK4+IW3MrtneQGia8EzgRuBw4W9KPcllPk5KZmfVA1Se42yYifgD8oK74cdKRi5mZdYAeXaA2M7O+wcnCzMxKVX350c5NyndsbThmZtaJqh5ZnNGk/M+tCsTMzDpXlxe4Ja2UOxeStCKgwuA1gDfaFZiZmXWOsruhJjH76emn64ZNA77f8ojMzKzjlCWLxUlHE9cDWxfKIyJ8VGFm1kdUeSgP4L3FcklDJb0ZEc+2LTIzM+sYVe+GOlXSlrl7b+AxYIKkT7czODMz6wxV74bantQkB6T2oj4MbAEc0Y6gzMyss1Rt7mPRiHhd0juAFSLieoB8h5SZmS3gqiaLeyV9HRgGXAFvJ4oZbYrLzMw6SNXTUJ8DRgArMPvU01bAOe0IyszMOkulI4uIeBj4RF3ZucC57QjKzMw6S+WGBCV9WtLlkm7P/e+X9PH2hWZmZp2i6q2z3wcOBs4D1srFzwHfbVNcZmbWQaoeWRwA7BARpzK7+Y/HSO1DmZnZAq5qsliE1BYUzE4WA4BXWx6RmZl1nKrJ4irgaEn9CmXfJ99Ga2ZmC7aqyeJgYG3S0cVSkqYC7wa+1a7AzMysc5S9z2JoREyKiKnADpJWBVYDJkbEhPkRoJmZ9b6y5yweAJaq9UTEU8BTbY3IzMw6TtlpKJUMNzOzPqDsyCIkiS6SRkTMam1IZmbWacqSxUDgzSbDRLqNtl+T4WZmtoAoSxavAuvPj0DMzKxzlSWLWRHx5HyJxMzMOpYvcJuZWamyZLH9fInCzMw6WpfJIiJunF+BmJlZ56r8PgszM+u7nCzMzKxUabKQ1E/SY5IWnR8BmZlZ5ylNFhHxFvAWsFj7wzEzs05U9pxFzfHAuZJ+Akxi9guQiIjH2xGYmZl1jqrJ4tf5/3Z15T1u7kPSMsBJwAZ5ep8FHgbOAYYBE4DdczPpZmbWCypd4I6IhZr8taJdqBOAv0fEusBGwIPA4cA1EbEWcE3uNzOzXtKtu6EkrSppS0mrtKJySUsBWwMnA0TEGxExDRgFnJZHOw3YpRX1mZnZvKmULCStKOl64FHgQuAxSTdIWqmH9a8BTAH+JOlOSSdJGgCsEBGTAfL/5ZvEdaCkcZLGTZkypYehmJlZM1WPLH4H3A0MiogVgUHAncDve1h/f+A9wO8iYhPgFbpxyikiToyI4RExfMiQIT0MxczMmql6gXsEsGJEzASIiFckfQt4uof1TwImRcStuf98UrJ4TtKKETFZ0orA8z2sx8zMeqDqkcVUYL26snWAaT2pPCKeBSZKWicXbUN67/elwH65bD/gkp7UY2ZmPVP1yOIY4GpJJwNPAqsBnwG+14IYvgKcKWkR4PE83YVIz3UcADwF7NaCeszMbB5VShYR8UdJjwGfAjYEngH2iohrexpARNwFDG8waJueTtvMzFqjabKQdEtEbJG7fxARPwR6nBzMzOx/T1fXLNaWVGsP6tD5EYyZmXWmrk5DXQL8W9IEYHFJNzQaKSK2bkdgZmbWOZomi4j4jKQRpPaZNiM/ZW1mZn1Plxe482tVb5S0SESc1tW4Zma24KrakOAp7Q7EzMw6l1+ramZmpZwszMyslJOFmZmVqtpEuSR9XtK1ku7JZVtL2r294ZmZWSeoemRxJHAAcCKwai6bBBzWjqDMzKyzVE0W+wM7RsTZpPdkAzxBenmRmZkt4Komi37AjNxdSxYDC2VmZrYAq5os/gYcJ2lRSNcwgKOAv7YrMDMz6xxVk8UhwErAS8DSpCOK1fA1CzOzPqH0fRb5KGIw8ElgWVKSmJjfcmdmZn1AabKIiJB0L7BkRDyP34dtZtbnVD0NdSewdjsDMTOzzlX1Hdxjgb9LOhWYyOw7otzIoJlZH1A1Wbyf9FzFB+rKA3CyMDNbwFVKFhHxwXYHYmZmnavqkQWSBgE7ASsDTwN/jYip7QrMzMw6R9WGBLcEHgMOAjYEvgA8lsvNzGwBV/XI4njg/3LbUABI2gP4Jen93GZmtgCreuvs2sC5dWXnA2u2NhwzM+tEVZPFI8CedWW7kU5NmZnZAq7qaaiDgcskfRV4EhgGrAXs2Ka4zMysg1S9dfZfkt4JfIzUoOBfgSsi4j/tDM7MzDpDpWQhaWXg1Yg4o1A2SNJKEfFM26IzM7OOUPWaxcXA0LqyocBFrQ3HzMw6UeW7oSLi3mJB7l+39SGZmVmnqZospkia4zbZ3P9i60MyM7NOUzVZnAJcIGlHSetJ2on0nMVJ7QvNzMw6RdVbZ8cAM4GfA6uQmik/CTiuTXGZmVkHqXrr7CzgZ/nPzMz6mC5PQ0laVdLQQv8Skn4s6RJJh0vq14ogJPWTdKeky3L/6pJulfSIpHMkLdKKeszMbN6UXbM4mTkbCvwNqdmPfwOfAY5qURxfAx4s9B8N/CIi1gKmAge0qB4zM5sHZcliI+AqAEkDgD2A3SPim8Ao5m4vqtvykcvHyBfLJQn4EOkCOsBpwC49rcfMzOZdWbJYJCJeyd2bAS9HxHiAiHgIGNyCGI4HvgXMyv3LAdMi4s3cP4n0wqW5SDpQ0jhJ46ZMmdKCUMzMrJGyZPGEpJG5e2fgutoASUOAV3tSuaQdgedrCahW3GDUaPT5iDgxIoZHxPAhQ4b0JBQzM+tC2d1Qo4GLJT1Oelp7ZGHYKOC2Htb/fmBnSTsAiwFLkY40lpHUPx9dDAXc/pSZWS/q8sgiIi4BNgV+DKwfEcXk8CDw7Z5UHhHfjoihETGMdP3j2ojYm3QE88k82n7AJT2px8zMeqb0OYuIeIwGLzmKiJvaElFyGHC2pB8Bd5LuyjIzs15S9QnutouIscDY3P04sHlvxmNmZrNVbRvKzMz6MCcLMzMrVSlZSPqqpFY8U2FmZv+Dqh5ZbAtMkHSZpD0kLdrOoMzMrLNUShYRsTOwGvA34GDgWUknSdq6ncGZmVlnqHzNIiJejIjfRMSWwAdIzX9cJ2mCpO9KGti2KM3MrFd16wK3pG0k/Yl0i+tzwL7Ap4FNSEcdZma2AKr0nIWkn5OesH4J+DNwREQ8XRh+C6kpcTMzWwBVfShvMeDjEXF7o4ERMVPS8NaFZWZmnaRqsvgpdS3MShoELB4Rz8DbTZabmdkCqOo1i4tJrb8WDQUuam04ZmbWiaomi3Ui4t5iQe5ft/UhmZlZp6maLJ6XtGaxIPe/2PqQzMys01RNFqcAF0jaUdJ6knYivSP7pPaFZmZmnaLqBe4xwEzg58AqwERSojiuTXGZmVkHqZQsImIW8LP8Z2ZmfUzllx9JWgRYBxgMqFYeEde2IS4zM+sgVZ/gHgGcBywKLAVMB5YknY5ao23RmZlZR6h6gfsXwDERsSzwcv5/FPDbtkVmZmYdo2qyWBs4oa5sDHBIa8MxM7NOVDVZvEQ6/QQwWdJ6wCDAzZKbmfUBVZPFhcAOuftk4DpgPOk6hpmZLeCq3jp7cKH7WEm3ki5wX9muwMzMrHOUJgtJ/YB/A+tFxOsAEXFjuwMzM7POUXoaKiLeAt4ivdPCzMz6oKoP5R0PnCvpJ8AkIGoDIuLxdgRmZmado2qy+HX+v11deQD9WheOmZl1oqoXuKveNWVmZgsgJwEzMytVtW2of1K4TlEUEVu3NCIzM+s4Va9Z1L/k6B3AAcAZrQ3HzMw6UdVrFqfVl0m6APgTcGSrgzIzs87Sk2sWTwMbtioQMzPrXFWvWXy2rmgJ4BPALS2PyMzMOk7Vaxafrut/BfgX6T0X80zSKsCfSddAZgEnRsQJkpYFzgGGAROA3SNiak/qMjOzeVf1msUH21T/m8ChEXGHpCWB8ZL+AewPXBMRYyQdDhwOHNamGMzMrESlaxaS9pW0YV3ZRpLqjzi6JSImR8Qduftl4EFgZWAUULuofhqwS0/qMTOznql6gfso0vu2iyYCP2pVIJKGAZsAtwIrRMRkSAkFWL7JZw6UNE7SuClTprQqFDMzq1M1WSwFTK8rewlYphVBSBoIXAAcHBH19TQVESdGxPCIGD5kyJBWhGJmZg1UTRYPALvWlX2cdNqoRyQtTEoUZ0bEhbn4OUkr5uErAs/3tB4zM5t3Ve+GOgy4QtIewGPAmsA2zH7V6jyRJNJrWh+MiOMKgy4F9gPG5P+X9KQeMzPrmUpHFvnNeOsDtwMDgNuADSLiph7W/37SbbkfknRX/tuBlCS2k/QIqVn0MT2sx8zMeqDqQ3mLAs9GxJhC2cKSFq29anVe5CSkJoO3mdfpmplZa1W9ZvEPYNO6sk2BK1sbjpmZdaKqyeLdpFtai24DNmptOGZm1omqJouXgBXqylYgNfthZmYLuKrJ4gLgL5I2kLSEpHeT2nQ6t32hmZlZp6iaLL5LeqbiNuBlUmuzDwPfaVNcZmbWQao2JPga8CVJXwYGAy9EREjyO7zNzPqAbv3YRzIF2EDSz4BJ7QnLzMw6SeVkIWmIpK9JugO4C9gc+FrbIjMzs47R5Wmo3G7TzqT3S3wEeBQ4C1gN2C0i3GaTmVkfUHZk8RzwB9LF7C0iYr2IOAp4o+2RmZlZxyhLFveQmiF/L7CZpEHtD8nMzDpNl8kiIkYC7wSuAr4BPCvpr6TGBBdue3RmZtYRSi9wR8STEXFURKxFatxvMjALuFvSMe0O0MzMel93b529MSIOBN4BfIXUZpSZmS3g5umhuoh4LSLOiojtWx2QmZl1Hj+BbWZmpZwszMyslJOFmZmVcrIwM7NSThZmZlbKycLMzEo5WZiZWSknCzMzK+VkYWZmpZwszMyslJOFmZmVcrIwM7NSThZmZlbKycLMzEo5WZiZWSknCzMzK+VkYWZmpZwszMyslJOFmZmVcrIwM7NSHZssJH1U0sOSHpV0eG/HY2bWl3VkspDUD/gNsD2wHrCXpPV6Nyozs76rI5MFsDnwaEQ8HhFvAGcDo3o5JjOzPqt/bwfQxMrAxEL/JOC99SNJOhA4MPfOkPTwfIitLxgMvNDbQXQKHd3bEVgDXkcLeriOrlZlpE5NFmpQFnMVRJwInNj+cPoWSeMiYnhvx2HWjNfR+a9TT0NNAlYp9A8FnumlWMzM+rxOTRa3A2tJWl3SIsCewKW9HJOZWZ/VkaehIuJNSV8GrgT6AadExP29HFZf4lN71um8js5nipjrUoCZmdkcOvU0lJmZdRAnCzMzK+VkUUJSSDq90N9f0hRJl1X47Iz8f5ikTxXKh0v6ZXsifruOncuaSZG0v6RfNymfJWnDQtl9koaVTO+kVjxpL2lsburlLkkP5udprISk70q6X9I9ednN9WzSfIzlYElLNCgfLemndWX8tQnVAAALf0lEQVQbS3pwHurYWNIOPYxzWN7GjyqUDZY0s9G2UXGaM5qUHyRp33mNtbc5WZR7BdhA0uK5fzvg6W5OYxjwdrKIiHER8dXWhNdYRFwaEWN6MIlJwHe7WefnIuKBHtRZtHdEbAy8Hzg63xVnTUjaEtgReE9EbAhsy5wPts7PWPoBBwNzJQvgLGCPurI9gb/MQ1UbA91KFpIa3dTzOGnZ1ewGtPyGmoj4fUT8udXTnV+cLKr5G/Cx3L0XaYUH3t5T+kahv9Ee+Bhgq7y3d4ikkbUjk/z5U/Le9OOSvlqY1tfz9O6TdHAuGybpobwXf5+kMyVtK+kmSY9I2jyP9/ZRg6SdJN0q6U5JV0taocI8XwasL2md+gGSfidpXN6L/WGhfGw+avqipGMK5ftL+lXu3kfSbXlZ/CH/sHRlIClhv9WsbknbSLqoUN92ki7M3R+WdLOkOySdJ2lgLh8j6YG8F/7zCsuj060IvBARrwNExAsR8QyApAmSBufu4ZLG5u7Rkk6XdG1edz6fy0dKukHSRXkZ/V7SQnnYXpLuzeve288NS5oh6UhJt5J2MlYCrpN0XTHIiHgYmFZ31LM7qUmfrr6vzST9S9Ldef1ZGjgS2COvS3tIWlbSxfk7vUX5yDjP54mSrgIa/Vj/F3hQUu0hvz2Acwvz1nD7kTRQ0p/y8rhH0q6Fz/w4x3pLYfy3fyvytnJ0npd/S9oql/eT9DNJt+dpfqH0m59fIsJ/XfwBM4ANgfOBxYC7gJHAZXn4aOAbhfHvA4bVPpv/vz1+fX/+/L+ARUlNGLwILAxsCtwLDCD9YN4PbEI6SnkTeDcp2Y8HTiE99T4KuDhPd3/g17l7ELPvfPsccGz9OHXzvD/wa2Bf4LQG87Vs/t8PGAtsmPvHAsOBIaS2vWrT+xswAngX8Fdg4Vz+W2DfBvWPBR4G7iFtyF8oDJur7jzvDwFD8rC/ADvl5XkDMCCXHwZ8H1g2T7+2TJbp7fWsBevpwLxu/jsv1w8Uhk0ABufu4cDYwrp3N7B4XlYTST/yI4HXgDXycv4H8Mk87Kn8/fYHrgV2ydMKYPdGdTaI9ZvAL3L3FsDtubvZ97UIae9/s1y+VK5/jvUX+BXwg9z9IeCuwnyOBxZvEMsw0rq9M/Bz0gPA11Bt+zkaOL4wrUGFZbFT7j4GOKL+tyKvu7Xp7ABcnbsPLIy/KDAOWL2316+I8JFFFRFxD2ml2gu4og1VXB4Rr0fEC8DzwAqkH9eLIuKViJgBXAhslcd/IiLujYhZpCRyTaS1694cZ72hwJWS7iVtqOtXjOsvwBaSVq8r313SHcCdeVpzXKeIiCnA45K2kLQcsA5wE7ANKQneLumu3L9Gk7r3jnQ6ZVXgG5Jq7dfMVXee99OBfSQtA2xJSlBb5NhuyvXtR2oHZzrpx/AkSZ8AXq24PDpWXkc2Jf3YTAHOkbR/hY9eEhH/zevedaRGPAFui9SQ51ukI+kRwGakRDMlIt4EzgS2zuO/BVxQMdyzgU/mo5U9mX2k3uz7WgeYHBG353mdnuuvN4K0HhAR1wLL5SMQgEsj4r9dxPR30inmvYBz6oY12362JbWOTa5zau58g3RkDilJDWtS54UNxvkwsG+e/1uB5YC1uoh7vunIh/I61KWkPY+RpC+w5k3mPJ232DxM+/VC91uk76VR+1iNxp9V6J9F4+/0V8BxEXGppJGkPZxSkR6OPJa0hwdAThzfIO3lTZV0Ko3n+RzS6YWHSEkvJIl0pPLtKvXnGKbk5PDe/OPSrO4/kY5aXgPOy7EL+EdE7FU/XaXTdduQfqy+TNoT/Z+Wf9jHAmPzD9t+wKnMuY7Wf1f1D1pFF+VdrZOv5fqrxDlR0gTgA8CupOROnv5c31c+nVTlgbCu2pR7pSSmNySNBw4lJYOdCoObbT9qEtfMvAMDs7fnRl5vMI6Ar0TElV3F2xt8ZFHdKcCREXFvXfkE4D0Akt4D1O+FA7wMLNnN+m4AdpG0hKQBwMeBf3ZzGjVLM/ui/H7d/OyppD2oIbl/KdKG91I+F7t9k89dCOzCnHtq15D2KJcHyOeYu2zxUumOmk2Ax7qqO9L5+WeAI3LMALcA75e0Zm1aktbO58GXjogrSBdiNy5fDJ1N0jqSinugGwNP5u4JpKMOSD/ORaMkLZaPAEeSmtoB2FypuZ2FSOfwbyTt6X5A6W6hfqTv9vomIZWt82cBvwAei4hJuazh90Xa4VhJ0ma5fEmlC9X1ddwA7J3HGUm6hjO9ixjqHQscFhEv1pU3236uIu1okOsc1I26mrkS+KKkhfM0187bf69zsqgoIiZFxAkNBl0ALJsPG79IOmdc7x7gzXzB65CK9d1B+tG7jbSRnhQRd85T8GlP6DxJ/6SbzTpHep/IL4Hlc//dpFNA95MS6E1NPjcVeABYLSJuy2UPkH7Mr5J0D+lc+IpNqj4zL9PxwKkRMb5C3WcCE3M9tdNh+wNn5fpuAdYl/cBclsuuByp9Jx1uIHCa8kV70umc0XnYD4ET8vdfv/d/G3A5adkclZMuwM2kGzPuA54gHR1OBr5NOl11N3BHRFzSJJ4Tgb/VX+AuOI+0B392raDZ95XXwT2AX0m6m7TeLJbjWK92gTvP7/D82TF0c8coIu6PiNMaDBpN4+3nR8AgpYv9dwMf7E59TZxE2m7ukHQf8Ac65AyQm/uwBYbS3V93RsTJvR3L/wJJo0k3Yfy8rnwk6ULsjo0+Z31TR2Qss57K55tfIZ1zNrMW85GFmZmV8jULMzMr5WRhZmalnCzMzKyUk4VZltvuOaON078/32mEkj9JmprbB9pK0sPtqtusp3w3lPU5Ss3Ff530zMXLpDaVftzueiOi2MzKCFLzEkMjovZ08VyNNpp1CicL61MkfR04HDiI9LTsG8BHSY0wdtkkRIutBkwoJIp5Jql/k7aSzFrGp6Gsz9DsZq2/FBEX5kYaZ0bEXyPimw3GP0/Ss5JeUmqye/3CsB3y09IvS3pas5ueHizpMknTJP1H0j81u3nvCUrNyR9AelJ3S6WmvX+o1Cz4pML0V5J0gdKLtp7QnE3Xj5Z0vqQzJE0H9pe0uVLT7dMlPSfpuLYtSOuTfGRhfcmWpGYiLiobMfsb8FnS0cfRpOZEau1InUxqkvufuU2gWptgh5JeHFVrS2sL6hqbi4iTJb0FfC4iRsDbT02TuxciNYp4Can9paHA1ZIeLjQwN4r0kp59SU1ZXwucEBGn57avNqg4j2aV+MjC+pLlSI3LVTplExGnRMTLkV4oNBrYSLObvJ5JapdoqYiYmtvyqpWvSGoTa2ZE/DO6/+TrZqR3cxwZEW9ExOPAH0kt5NbcHBEXR8Ss3PT2TGBNSYMjYkZE3NLNOs265GRhfcmLwGA1frXmHJTeWDZG0mP5VM+EPGhw/r8r6aU1T0q6Xum1pgA/Ax4lNZb4uEreg97EaqRWVqfV/oDvkN5zUlP/ytQDgLWBh5TesuZ2naylnCysL7mZ9L6LXSqM+ynSqZ5tSU1UD8vlAoiI2yNiFKk13ovJr+HMRyKHRsQapHcifF3SNt2McyLpBVfLFP6WjIji+6brT209kt8DsTzplNn5ndK0tS0YnCysz4iIl0iv6fyNpNq7QhaWtL0K7wzPliS9nOZFYAngJ7UBkhaRtLekpSNiJunNe7V3hO8oaU1JKpRXeilQwW3AdEmHSVo8H+VsoPw+h0aU3m0+JL89cVou7m69Zk05WVifEhHHkZ6xOIL0+tGJpBfYXFw36p9JLw96mvR+gfprAJ8GJuRTVAcB++TytYCrSe9uvxn4bUSM7WaMb5GOSjYmvUviBdLdU0t38bGPAvdLmgGcAOwZEa91p16zrrjVWTMzK+UjCzMzK+VkYWZmpZwszMyslJOFmZmVcrIwM7NSThZmZlbKycLMzEo5WZiZWan/B4bskPlLRdITAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def compareResults(classifiers,accuracies, setType, scoreName):\n",
    "    \n",
    "    index = np.arange(len(classifiers))\n",
    "    plot.bar(index, accuracies)\n",
    "    plot.xlabel('Classifiers', fontsize=12)\n",
    "    plot.ylabel(scoreName + ' Score for '+ setType +' Set', fontsize=12)\n",
    "    plot.xticks(index, classifiers, fontsize=10)\n",
    "    plot.title(scoreName + ' Score Comparison of Classifiers on '+ setType +' Set')\n",
    "    plot.show()\n",
    "\n",
    "classifiersList = [\"Multinomial Naive Bayes\", \"Support Vector Machine\"]    \n",
    "testAccuracies = [accTestNB,accTestSVM]\n",
    "compareResults(classifiers=classifiersList, accuracies=testAccuracies, setType=\"Test\",scoreName=\"Accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change in SVM Kernels to check the effect on Accuracy and Confusion\n",
    "\n",
    "* From the above graph , it is clearly seen that the Linear Kernel is well suited for text classification. Most importantly The Linear Kernel is computationally very cheap when compared to other Kernels.\n",
    "* The linear kernel is good when there is a lot of features. That's because mapping the data to a higher dimensional space does not really improve the performance.\n",
    "* SVM kernels are data dependent which means Linear SVMs works well with the linearly separable data. When you train a SVM with a linear kernel, you only need to optimize the C regularization parameter.So for choosing the best parameters we need to use GridSearchCV which is time consuming and not good performancewise.\n",
    "* When you train a SVM with other kernels such as Polynomial Kernel, Radial Basis Function (RBF), Sigmoid Kernel , it is quite time consuming and also decrease the accuracy to ~29%. Trying those kernels also increase the confusion as shown in the confusion matrix.\n",
    "* Tuning of the Regularization parameter C and othe parameters like degree is important as High C values tend to overfit. So it is recommended to use the low C value for the better accuracy and performance.\n",
    "\n",
    "Sources : [Various Kernels Discussion :](https://www.researchgate.net/post/Is_it_necessary_to_choose_kernels_in_SVM_according_to_application)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Polynomial Kernel \n",
      "\n",
      "\t \t Accuracy of Support Vector Machine Classifier  on training set: 29.535864978902953\n",
      "\t \t Accuracy of Support Vector Machine Classifier  on Test set: 28.220255653883974 \n",
      "\n",
      "                    alt.atheism  comp.graphics  sci.space  talk.religion.misc\n",
      "alt.atheism                   0              0        253                   0\n",
      "comp.graphics                 0              0        296                   0\n",
      "sci.space                     0              0        287                   0\n",
      "talk.religion.misc            0              0        181                   0\n",
      "Using RBF Kernel \n",
      "\n",
      "\t \t Accuracy of Support Vector Machine Classifier  on training set: 29.535864978902953\n",
      "\t \t Accuracy of Support Vector Machine Classifier  on Test set: 28.220255653883974 \n",
      "\n",
      "                    alt.atheism  comp.graphics  sci.space  talk.religion.misc\n",
      "alt.atheism                   0              0        253                   0\n",
      "comp.graphics                 0              0        296                   0\n",
      "sci.space                     0              0        287                   0\n",
      "talk.religion.misc            0              0        181                   0\n",
      "Using Sigmoid Kernel \n",
      "\n",
      "\t \t Accuracy of Support Vector Machine Classifier  on training set: 29.535864978902953\n",
      "\t \t Accuracy of Support Vector Machine Classifier  on Test set: 28.220255653883974 \n",
      "\n",
      "                    alt.atheism  comp.graphics  sci.space  talk.religion.misc\n",
      "alt.atheism                   0              0        253                   0\n",
      "comp.graphics                 0              0        296                   0\n",
      "sci.space                     0              0        287                   0\n",
      "talk.religion.misc            0              0        181                   0\n"
     ]
    }
   ],
   "source": [
    "def svmKernal(kernal_name):\n",
    "    \n",
    "    SVM_clf  = svm.SVC( kernel=kernal_name,C=10, degree=3, gamma='auto')\n",
    "    cm_trainSVM,cm_test_SVM,accTrainSVM,accTestSVM = classifyDatasets(classifier=SVM_clf,\n",
    "                                                         classifierName=\"Support Vector Machine Classifier \")\n",
    "    print(cm_test_SVM)\n",
    "    return cm_test_SVM\n",
    "    \n",
    "print(\"Using Polynomial Kernel \\n\")\n",
    "cm_test_SVM_poly = svmKernal(kernal_name = 'poly')\n",
    "print(\"Using RBF Kernel \\n\")\n",
    "cm_test_SVM_string = svmKernal(kernal_name = 'rbf')\n",
    "print(\"Using Sigmoid Kernel \\n\")\n",
    "cm_test_SVM_sig = svmKernal(kernal_name = 'sigmoid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-2 d)\n",
    "### POS Tagging in Raw Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenizing Raw Data \n",
    "\n",
    "We have tokenized the entire raw dataset using word_tokenize() which will be applied to each and every sentence and tokenize it. We are displaying the some of words in list to demonstrate from entire list of tokenized words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['From', ':', 'healta', '@', 'saturn.wwc.edu', '(', 'Tammy', 'R', 'Healy', ')', 'Subject', ':', 'Re', ':', 'who', 'are', 'we', 'to', 'judge', ',', 'Bobby', '?', 'Lines', ':', '38', 'Organization', ':', 'Walla', 'Walla', 'College', 'Lines', ':', '38', 'In', 'article', '<', '1993Apr14.213356.22176', '@', 'ultb.isc.rit.edu', '>', 'snm6394', '@', 'ultb.isc.rit.edu', '(', 'S.N', '.', 'Mozumder', ')', 'writes', ':', '>', 'From', ':', 'snm6394', '@', 'ultb.isc.rit.edu', '(', 'S.N', '.', 'Mozumder', ')', '>', 'Subject', ':', 'Re', ':', 'who', 'are', 'we', 'to', 'judge', ',', 'Bobby', '?', '>', 'Date', ':', 'Wed', ',', '14', 'Apr', '1993', '21:33:56', 'GMT', '>', 'In', 'article', '<', 'healta.56.734556346', '@', 'saturn.wwc.edu', '>', 'healta', '@', 'saturn.wwc.edu', '(', 'TAMMY', 'R', 'HEALY', ')', 'writes', ':', '>', '>', 'Bobby', ',', '>', '>', '>', '>', 'I', 'would', 'like', 'to', 'take', 'the', 'liberty', 'to', 'quote', 'from', 'a', 'Christian', 'writer', 'named', '>', '>', 'Ellen', 'G.', 'White', '.', 'I', 'hope', 'that', 'what', 'she', 'said', 'will', 'help', 'you', 'to', 'edit', 'your', '>', '>', 'remarks', 'in', 'this', 'group', 'in', 'the', 'future', '.', '>', '>', '>', '>', \"''\", 'Do', 'not', 'set', 'yourself', 'as', 'a', 'standard', '.', 'Do', 'not', 'make', 'your', 'opinions', ',', 'your', 'views', '>', '>', 'of', 'duty', ',', 'your', 'interpretations', 'of', 'scripture', ',', 'a', 'criterion', 'for', 'others', 'and', 'in', '>', '>', 'your', 'heart', 'condemn', 'them', 'if', 'they', 'do', 'not', 'come', 'up', 'to', 'your', 'ideal', '.', \"''\", '>', '>', 'Thoughts', 'Fromthe', 'Mount', 'of', 'Blessing', 'p.', '124', '>', '>', '>', '>', 'I', 'hope', 'quoting', 'this', 'does', \"n't\", 'make', 'the', 'atheists', 'gag', ',', 'but', 'I', 'think', 'Ellen', 'White', '>', '>', 'put', 'it', 'better', 'than', 'I', 'could', '.', '>', '>', '>', '>', 'Tammy', '>', '>', 'Point', '?', '>', '>', 'Peace', ',', '>', '>', 'Bobby', 'Mozumder', '>', 'My', 'point', 'is', 'that', 'you', 'set', 'up', 'your', 'views', 'as', 'the', 'only', 'way', 'to', 'believe', '.', 'Saying', 'that', 'all', 'eveil', 'in', 'this', 'world', 'is', 'caused', 'by', 'atheism', 'is', 'ridiculous', 'and', 'counterproductive', 'to', 'dialogue', 'in', 'this', 'newsgroups', '.', 'I', 'see', 'in', 'your', 'posts', 'a', 'spirit', 'of', 'condemnation', 'of', 'the', 'atheists', 'in', 'this', 'newsgroup', 'bacause', 'they', \"don'\", 't', 'believe', 'exactly', 'as', 'you', 'do', '.', 'If', 'you', \"'re\", 'here', 'to', 'try', 'to', 'convert', 'the', 'atheists', 'here', ',', 'you', \"'re\", 'failing', 'miserably', '.', 'Who', 'wants', 'to', 'be', 'in', 'position', 'of', 'constantly', 'defending', 'themselves', 'agaist', 'insulting', 'attacks', ',', 'like', 'you', 'seem', 'to', 'like', 'to', 'do', '?', '!', 'I', \"'m\", 'sorry', 'you', \"'re\", 'so', 'blind', 'that', 'you', 'did', \"n't\", 'get', 'the', 'messgae', 'in', 'the', 'quote', ',', 'everyone', 'else', 'has', 'seemed', 'to', '.', 'Tammy']\n"
     ]
    }
   ],
   "source": [
    "def tokenizeRawData():\n",
    "\n",
    "    tokenized_word_raw = []\n",
    "    for i in newsgroups_All.data:\n",
    "        tokenized_word_raw.append(word_tokenize(i))\n",
    "    return tokenized_word_raw\n",
    "\n",
    "tokenized_word_raw = tokenizeRawData()\n",
    "print(tokenized_word_raw[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### POS tagging\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('From', 'IN'), (':', ':'), ('healta', 'NN'), ('@', 'NN'), ('saturn.wwc.edu', 'NN'), ('(', '('), ('Tammy', 'NNP'), ('R', 'NNP'), ('Healy', 'NNP'), (')', ')'), ('Subject', 'NN'), (':', ':'), ('Re', 'NN'), (':', ':'), ('who', 'WP'), ('are', 'VBP'), ('we', 'PRP'), ('to', 'TO'), ('judge', 'VB'), (',', ','), ('Bobby', 'NNP'), ('?', '.'), ('Lines', 'NNS'), (':', ':'), ('38', 'CD'), ('Organization', 'NN'), (':', ':'), ('Walla', 'NNP'), ('Walla', 'NNP'), ('College', 'NNP'), ('Lines', 'NNP'), (':', ':'), ('38', 'CD'), ('In', 'IN'), ('article', 'NN'), ('<', '$'), ('1993Apr14.213356.22176', 'CD'), ('@', 'NNP'), ('ultb.isc.rit.edu', 'JJ'), ('>', 'NNP'), ('snm6394', 'NN'), ('@', 'NNP'), ('ultb.isc.rit.edu', 'NN'), ('(', '('), ('S.N', 'NNP'), ('.', '.'), ('Mozumder', 'NNP'), (')', ')'), ('writes', 'VBZ'), (':', ':'), ('>', 'NN'), ('From', 'IN'), (':', ':'), ('snm6394', 'NN'), ('@', 'NN'), ('ultb.isc.rit.edu', 'NN'), ('(', '('), ('S.N', 'NNP'), ('.', '.'), ('Mozumder', 'NNP'), (')', ')'), ('>', 'NN'), ('Subject', 'JJ'), (':', ':'), ('Re', 'NN'), (':', ':'), ('who', 'WP'), ('are', 'VBP'), ('we', 'PRP'), ('to', 'TO'), ('judge', 'VB'), (',', ','), ('Bobby', 'NNP'), ('?', '.'), ('>', 'NN'), ('Date', 'NN'), (':', ':'), ('Wed', 'NNP'), (',', ','), ('14', 'CD'), ('Apr', 'NNP'), ('1993', 'CD'), ('21:33:56', 'CD'), ('GMT', 'NNP'), ('>', 'NN'), ('In', 'IN'), ('article', 'NN'), ('<', 'NNP'), ('healta.56.734556346', 'NN'), ('@', 'NNP'), ('saturn.wwc.edu', 'NN'), ('>', 'NNP'), ('healta', 'NN'), ('@', 'NNP'), ('saturn.wwc.edu', 'NN'), ('(', '('), ('TAMMY', 'NNP'), ('R', 'NNP'), ('HEALY', 'NNP'), (')', ')'), ('writes', 'VBZ'), (':', ':'), ('>', 'NN'), ('>', 'NN'), ('Bobby', 'NNP'), (',', ','), ('>', 'NNP'), ('>', 'NNP'), ('>', 'NNP'), ('>', 'NNP'), ('I', 'PRP'), ('would', 'MD'), ('like', 'VB'), ('to', 'TO'), ('take', 'VB'), ('the', 'DT'), ('liberty', 'NN'), ('to', 'TO'), ('quote', 'VB'), ('from', 'IN'), ('a', 'DT'), ('Christian', 'JJ'), ('writer', 'NN'), ('named', 'VBN'), ('>', 'NNP'), ('>', 'NNP'), ('Ellen', 'NNP'), ('G.', 'NNP'), ('White', 'NNP'), ('.', '.'), ('I', 'PRP'), ('hope', 'VBP'), ('that', 'IN'), ('what', 'WP'), ('she', 'PRP'), ('said', 'VBD'), ('will', 'MD'), ('help', 'VB'), ('you', 'PRP'), ('to', 'TO'), ('edit', 'VB'), ('your', 'PRP$'), ('>', 'NN'), ('>', 'NN'), ('remarks', 'NNS'), ('in', 'IN'), ('this', 'DT'), ('group', 'NN'), ('in', 'IN'), ('the', 'DT'), ('future', 'NN'), ('.', '.'), ('>', 'JJ'), ('>', 'JJ'), ('>', 'NN'), ('>', 'NN'), (\"''\", \"''\"), ('Do', 'NNP'), ('not', 'RB'), ('set', 'VB'), ('yourself', 'PRP'), ('as', 'IN'), ('a', 'DT'), ('standard', 'NN'), ('.', '.'), ('Do', 'VBP'), ('not', 'RB'), ('make', 'VB'), ('your', 'PRP$'), ('opinions', 'NNS'), (',', ','), ('your', 'PRP$'), ('views', 'NNS'), ('>', 'VBP'), ('>', 'CD'), ('of', 'IN'), ('duty', 'NN'), (',', ','), ('your', 'PRP$'), ('interpretations', 'NNS'), ('of', 'IN'), ('scripture', 'NN'), (',', ','), ('a', 'DT'), ('criterion', 'NN'), ('for', 'IN'), ('others', 'NNS'), ('and', 'CC'), ('in', 'IN'), ('>', 'NNP'), ('>', 'NNP'), ('your', 'PRP$'), ('heart', 'NN'), ('condemn', 'VB'), ('them', 'PRP'), ('if', 'IN'), ('they', 'PRP'), ('do', 'VBP'), ('not', 'RB'), ('come', 'VB'), ('up', 'RP'), ('to', 'TO'), ('your', 'PRP$'), ('ideal', 'NN'), ('.', '.'), (\"''\", \"''\"), ('>', 'JJ'), ('>', 'NNP'), ('Thoughts', 'NNP'), ('Fromthe', 'NNP'), ('Mount', 'NNP'), ('of', 'IN'), ('Blessing', 'NNP'), ('p.', 'NN'), ('124', 'CD'), ('>', 'NN'), ('>', 'NNP'), ('>', 'NNP'), ('>', 'NNP'), ('I', 'PRP'), ('hope', 'VBP'), ('quoting', 'VBG'), ('this', 'DT'), ('does', 'VBZ'), (\"n't\", 'RB'), ('make', 'VB'), ('the', 'DT'), ('atheists', 'NNS'), ('gag', 'VBP'), (',', ','), ('but', 'CC'), ('I', 'PRP'), ('think', 'VBP'), ('Ellen', 'NNP'), ('White', 'NNP'), ('>', 'NNP'), ('>', 'NNP'), ('put', 'VBD'), ('it', 'PRP'), ('better', 'JJR'), ('than', 'IN'), ('I', 'PRP'), ('could', 'MD'), ('.', '.'), ('>', 'VB'), ('>', 'JJ'), ('>', 'NNP'), ('>', 'NNP'), ('Tammy', 'NNP'), ('>', 'NNP'), ('>', 'NNP'), ('Point', 'NNP'), ('?', '.'), ('>', 'NNP'), ('>', 'NNP'), ('Peace', 'NNP'), (',', ','), ('>', 'NNP'), ('>', 'NNP'), ('Bobby', 'NNP'), ('Mozumder', 'NNP'), ('>', 'NNP'), ('My', 'NNP'), ('point', 'NN'), ('is', 'VBZ'), ('that', 'IN'), ('you', 'PRP'), ('set', 'VBP'), ('up', 'RP'), ('your', 'PRP$'), ('views', 'NNS'), ('as', 'IN'), ('the', 'DT'), ('only', 'JJ'), ('way', 'NN'), ('to', 'TO'), ('believe', 'VB'), ('.', '.'), ('Saying', 'VBG'), ('that', 'IN'), ('all', 'DT'), ('eveil', 'NN'), ('in', 'IN'), ('this', 'DT'), ('world', 'NN'), ('is', 'VBZ'), ('caused', 'VBN'), ('by', 'IN'), ('atheism', 'NN'), ('is', 'VBZ'), ('ridiculous', 'JJ'), ('and', 'CC'), ('counterproductive', 'JJ'), ('to', 'TO'), ('dialogue', 'VB'), ('in', 'IN'), ('this', 'DT'), ('newsgroups', 'NN'), ('.', '.'), ('I', 'PRP'), ('see', 'VBP'), ('in', 'IN'), ('your', 'PRP$'), ('posts', 'NNS'), ('a', 'DT'), ('spirit', 'NN'), ('of', 'IN'), ('condemnation', 'NN'), ('of', 'IN'), ('the', 'DT'), ('atheists', 'NNS'), ('in', 'IN'), ('this', 'DT'), ('newsgroup', 'NN'), ('bacause', 'IN'), ('they', 'PRP'), (\"don'\", 'VBP'), ('t', 'JJ'), ('believe', 'VBP'), ('exactly', 'RB'), ('as', 'IN'), ('you', 'PRP'), ('do', 'VBP'), ('.', '.'), ('If', 'IN'), ('you', 'PRP'), (\"'re\", 'VBP'), ('here', 'RB'), ('to', 'TO'), ('try', 'VB'), ('to', 'TO'), ('convert', 'VB'), ('the', 'DT'), ('atheists', 'NNS'), ('here', 'RB'), (',', ','), ('you', 'PRP'), (\"'re\", 'VBP'), ('failing', 'VBG'), ('miserably', 'RB'), ('.', '.'), ('Who', 'WP'), ('wants', 'VBZ'), ('to', 'TO'), ('be', 'VB'), ('in', 'IN'), ('position', 'NN'), ('of', 'IN'), ('constantly', 'RB'), ('defending', 'VBG'), ('themselves', 'PRP'), ('agaist', 'JJ'), ('insulting', 'VBG'), ('attacks', 'NNS'), (',', ','), ('like', 'IN'), ('you', 'PRP'), ('seem', 'VBP'), ('to', 'TO'), ('like', 'VB'), ('to', 'TO'), ('do', 'VB'), ('?', '.'), ('!', '.'), ('I', 'PRP'), (\"'m\", 'VBP'), ('sorry', 'JJ'), ('you', 'PRP'), (\"'re\", 'VBP'), ('so', 'RB'), ('blind', 'IN'), ('that', 'IN'), ('you', 'PRP'), ('did', 'VBD'), (\"n't\", 'RB'), ('get', 'VB'), ('the', 'DT'), ('messgae', 'NN'), ('in', 'IN'), ('the', 'DT'), ('quote', 'NN'), (',', ','), ('everyone', 'NN'), ('else', 'RB'), ('has', 'VBZ'), ('seemed', 'VBN'), ('to', 'TO'), ('.', '.'), ('Tammy', 'VB')]\n"
     ]
    }
   ],
   "source": [
    "def posTaggingRawData(tokenized_word_raw):\n",
    "\n",
    "    pos_tag_raw = []\n",
    "    for i in range(len(tokenized_word_raw)):\n",
    "        pos_tag_raw.append(nltk.pos_tag(tokenized_word_raw[i]))\n",
    "        \n",
    "    return pos_tag_raw\n",
    "\n",
    "pos_tag_raw = posTaggingRawData(tokenized_word_raw)\n",
    "print(pos_tag_raw[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract the Noun\n",
    "We have extracted all the noun using the below function from the Dataset.\n",
    "For cleaning we have performed the same process which we used in the Question 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['healta', '@', 'saturn.wwc.edu', 'Tammy', 'R', 'Healy', 'Subject', 'Re', 'Bobby', 'Lines', 'Organization', 'Walla', 'Walla', 'College', 'Lines', 'article', '@', '>', 'snm6394', '@', 'ultb.isc.rit.edu', 'S.N', 'Mozumder', '>', 'snm6394', '@', 'ultb.isc.rit.edu', 'S.N', 'Mozumder', '>', 'Re', 'Bobby', '>', 'Date', 'Wed', 'Apr', 'GMT', '>', 'article', '<', 'healta.56.734556346', '@', 'saturn.wwc.edu', '>', 'healta', '@', 'saturn.wwc.edu', 'TAMMY', 'R', 'HEALY', '>', '>', 'Bobby', '>', '>', '>', '>', 'liberty', 'writer', '>', '>', 'Ellen', 'G.', 'White', '>', '>', 'remarks', 'group', 'future', '>', '>', 'Do', 'standard', 'opinions', 'views', 'duty', 'interpretations', 'scripture', 'criterion', 'others', '>', '>', 'heart', 'ideal', '>', 'Thoughts', 'Fromthe', 'Mount', 'Blessing', 'p.', '>', '>', '>', '>', 'atheists', 'Ellen', 'White', '>', '>', '>', '>', 'Tammy', '>', '>', 'Point', '>', '>', 'Peace', '>', '>', 'Bobby', 'Mozumder', '>', 'My', 'point', 'views', 'way', 'eveil', 'world', 'atheism', 'newsgroups', 'posts', 'spirit', 'condemnation', 'atheists', 'newsgroup', 'atheists', 'position', 'attacks', 'messgae', 'quote', 'everyone']\n"
     ]
    }
   ],
   "source": [
    "def extractNouns(pos_tag_raw):\n",
    "    \n",
    "    l1=[]\n",
    "    final_pos=[]\n",
    "    new_list=[]\n",
    "    for i in range(len(pos_tag_raw)):\n",
    "        l1 = pos_tag_raw[i]\n",
    "        df=pd.DataFrame(l1)\n",
    "        l2=df[0].tolist()\n",
    "        l3=df[1].tolist()\n",
    "        for j in range(len(l2)):\n",
    "            if l3[j] == 'NN' or l3[j] == 'NNS' or l3[j] == 'NNP' or l3[j] == 'NNPS' :\n",
    "                new_list.append(l2[j])\n",
    "        final_pos.append(new_list)\n",
    "        new_list=[]\n",
    "    \n",
    "    return final_pos\n",
    "\n",
    "final_pos = extractNouns(pos_tag_raw)\n",
    "print(final_pos[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['healta', '', 'saturnwwcedu', 'Tammy', 'R', 'Healy', 'Subject', 'Re', 'Bobby', 'Lines', 'Organization', 'Walla', 'Walla', 'College', 'Lines', 'article', '', '', 'snm', '', 'ultbiscritedu', 'SN', 'Mozumder', '', 'snm', '', 'ultbiscritedu', 'SN', 'Mozumder', '', 'Re', 'Bobby', '', 'Date', 'Wed', 'Apr', 'GMT', '', 'article', '', 'healta', '', 'saturnwwcedu', '', 'healta', '', 'saturnwwcedu', 'TAMMY', 'R', 'HEALY', '', '', 'Bobby', '', '', '', '', 'liberty', 'writer', '', '', 'Ellen', 'G', 'White', '', '', 'remarks', 'group', 'future', '', '', 'Do', 'standard', 'opinions', 'views', 'duty', 'interpretations', 'scripture', 'criterion', 'others', '', '', 'heart', 'ideal', '', 'Thoughts', 'Fromthe', 'Mount', 'Blessing', 'p', '', '', '', '', 'atheists', 'Ellen', 'White', '', '', '', '', 'Tammy', '', '', 'Point', '', '', 'Peace', '', '', 'Bobby', 'Mozumder', '', 'My', 'point', 'views', 'way', 'eveil', 'world', 'atheism', 'newsgroups', 'posts', 'spirit', 'condemnation', 'atheists', 'newsgroup', 'atheists', 'position', 'attacks', 'messgae', 'quote', 'everyone']\n"
     ]
    }
   ],
   "source": [
    "def removeNoncharcterPos(final_pos):\n",
    "\n",
    "    Remove_noncharacter_pos=[]\n",
    "    Remove_noncharacter_final_pos=[]\n",
    "    for i in range(len(final_pos)):\n",
    "        for j in range(len(final_pos[i])):\n",
    "            Remove_noncharacter_pos.append(re.sub(r'[^A-Za-z]', '', final_pos[i][j]))\n",
    "        Remove_noncharacter_final_pos.append(Remove_noncharacter_pos)\n",
    "        Remove_noncharacter_pos=[]\n",
    "        \n",
    "    return Remove_noncharacter_final_pos\n",
    "\n",
    "Remove_noncharacter_final_pos = removeNoncharcterPos(final_pos)    \n",
    "print(Remove_noncharacter_final_pos[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['healta', 'saturnwwcedu', 'Tammy', 'R', 'Healy', 'Subject', 'Re', 'Bobby', 'Lines', 'Organization', 'Walla', 'Walla', 'College', 'Lines', 'article', 'snm', 'ultbiscritedu', 'SN', 'Mozumder', 'snm', 'ultbiscritedu', 'SN', 'Mozumder', 'Re', 'Bobby', 'Date', 'Wed', 'Apr', 'GMT', 'article', 'healta', 'saturnwwcedu', 'healta', 'saturnwwcedu', 'TAMMY', 'R', 'HEALY', 'Bobby', 'liberty', 'writer', 'Ellen', 'G', 'White', 'remarks', 'group', 'future', 'Do', 'standard', 'opinions', 'views', 'duty', 'interpretations', 'scripture', 'criterion', 'others', 'heart', 'ideal', 'Thoughts', 'Fromthe', 'Mount', 'Blessing', 'p', 'atheists', 'Ellen', 'White', 'Tammy', 'Point', 'Peace', 'Bobby', 'Mozumder', 'My', 'point', 'views', 'way', 'eveil', 'world', 'atheism', 'newsgroups', 'posts', 'spirit', 'condemnation', 'atheists', 'newsgroup', 'atheists', 'position', 'attacks', 'messgae', 'quote', 'everyone']\n"
     ]
    }
   ],
   "source": [
    "def removeNullsPos(Remove_noncharacter_final_pos):\n",
    "    \n",
    "    Remove_null_pos=[]\n",
    "    for i in range(len(Remove_noncharacter_final_pos)):\n",
    "        Remove_null_pos.append([x for x in Remove_noncharacter_final_pos[i] if x])\n",
    "   \n",
    "    return Remove_null_pos\n",
    "\n",
    "Remove_null_pos = removeNullsPos(Remove_noncharacter_final_pos)\n",
    "print(Remove_null_pos[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['healta', 'saturnwwcedu', 'tammi', 'R', 'heali', 'subject', 'Re', 'bobbi', 'line', 'organ', 'walla', 'walla', 'colleg', 'line', 'articl', 'snm', 'ultbiscritedu', 'SN', 'mozumd', 'snm', 'ultbiscritedu', 'SN', 'mozumd', 'Re', 'bobbi', 'date', 'wed', 'apr', 'gmt', 'articl', 'healta', 'saturnwwcedu', 'healta', 'saturnwwcedu', 'tammi', 'R', 'heali', 'bobbi', 'liberti', 'writer', 'ellen', 'G', 'white', 'remark', 'group', 'futur', 'Do', 'standard', 'opinion', 'view', 'duti', 'interpret', 'scriptur', 'criterion', 'other', 'heart', 'ideal', 'thought', 'fromth', 'mount', 'bless', 'p', 'atheist', 'ellen', 'white', 'tammi', 'point', 'peac', 'bobbi', 'mozumd', 'My', 'point', 'view', 'way', 'eveil', 'world', 'atheism', 'newsgroup', 'post', 'spirit', 'condemn', 'atheist', 'newsgroup', 'atheist', 'posit', 'attack', 'messga', 'quot', 'everyon']\n"
     ]
    }
   ],
   "source": [
    "def stemmingProcessPos(Remove_null_pos):\n",
    "    \n",
    "    stemmed_words_pos=[]\n",
    "    temp_stemmed_words_pos=[]\n",
    "\n",
    "    for i in range(len(Remove_null_pos)):\n",
    "        for j in range(len(Remove_null_pos[i])):\n",
    "            temp_stemmed_words_pos.append(PorterStemmer().stem(Remove_null_pos[i][j]))\n",
    "        stemmed_words_pos.append(temp_stemmed_words_pos)\n",
    "        temp_stemmed_words_pos=[]\n",
    "        \n",
    "    return stemmed_words_pos\n",
    "\n",
    "stemmed_words_pos = stemmingProcessPos(Remove_null_pos)\n",
    "print(stemmed_words_pos[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data_pos=[]\n",
    "\n",
    "for i in range(len(stemmed_words_pos)):\n",
    "    clean_data_pos.append(' '.join(stemmed_words_pos[i]))\n",
    "    \n",
    "#print(clean_data_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Data Representation : Bag of Words and TF - IDF\n",
    "\n",
    "Explanation of the code and theory for Data representation techniques, Bag of Words and TF-IDF, will be the same as   explained in Answer 2(b)\n",
    "\n",
    "#### Vocabulary Comparison\n",
    "Vocabulary is seen decreased clearly from the shape attribute. Here it shows tuple (3387,25229) which  means 3387 documents and 25229 words. In previous Question that tuple was (3387,31715) which shows the 3387 documents and 31715 words. These results show that size of words is decreased after cleaning and  extracting only nouns from the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3387, 25233)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_counts = convertToBagOfWords(clean_data_pos)\n",
    "X_train_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3387, 25233)"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tfidf =convertToTfIdf(X_train_counts)\n",
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Classification in Data Represented in Bag Of Words and TF-IDF\n",
    "\n",
    "Data split and classification explanation will be the same as explained in Answer 2(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test,y_train,y_test = train_test_split(X_train_tfidf,newsgroups_All.target,test_size = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t \t Accuracy of Multinomial Naive Bayes Classifier  on training set: 96.41350210970464\n",
      "\t \t Accuracy of Multinomial Naive Bayes Classifier  on Test set: 93.41199606686332 \n",
      "\n",
      "\t \t Accuracy of Support Vector Machine Classifier  on training set: 99.40928270042194\n",
      "\t \t Accuracy of Support Vector Machine Classifier  on Test set: 95.37856440511308 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "multinomialNBClf = MultinomialNB()\n",
    "cm_trainNB,cm_testNB,newAccTrainNB,newAccTestNB = classifyDatasets(classifier=multinomialNBClf,\n",
    "                                                         classifierName=\"Multinomial Naive Bayes Classifier \")\n",
    "\n",
    "svmClf = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
    "cm_trainSVM,cm_test_SVM,newAccTrainSVM,newAccTestSVM = classifyDatasets(classifier=svmClf,\n",
    "                                                         classifierName=\"Support Vector Machine Classifier \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy Comparison compared to that of answer 2(c)\n",
    "\n",
    "Accuracy remains almost the same for extracting nouns and other types words from data and extracting only nouns from data which can be easily seen from the Accuracy comparison bar graph.\n",
    "To be precise, we can say that there is slight increase in the accuracy of SVM and Multinomial Naive Bayes Classifier. \n",
    "\n",
    "### Note:- \n",
    "We have used functions to perform all the operations in this Assignment.\n",
    "This way re-usability of the code is improved.\n",
    "For ex. To create a model and compare the accuracy in the Q-2 d) we have directly called the functions which were\n",
    "previously created to create model and find the score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def accuracyComprison(classifiers,accuracies,accuraciesNew):\n",
    "\n",
    "    index = np.arange(len(classifiers))\n",
    "    bar_width = 0.25\n",
    "    legend = ['Old Accuracy Scores','New Accuracy Scores']\n",
    "    plot.bar(index, accuracies, bar_width,color='red',label='Old Accuracy Scores')\n",
    "    plot.bar(index + bar_width, accuraciesNew, bar_width,color='green',label='New Accuracy Scores')\n",
    "    plot.xlabel('Classifiers',fontsize=15)\n",
    "    plot.ylabel('Accuracy Scores',fontsize=15)\n",
    "    plot.title('Comparison of Test Accuracies Before and After Extracting Only Nouns',fontsize=15)\n",
    "    plot.xticks(index + bar_width, classifiers)\n",
    "    plot.legend(legend,loc=7,bbox_to_anchor=(1.45, 0.8))\n",
    "    plot.show()\n",
    "\n",
    "listOfTestAccuracies  = [accTestNB,accTestSVM]\n",
    "listTestAccNew = [newAccTestNB,newAccTestSVM]    \n",
    "listOfClassifiers = [\"Multinomial Naive Bayes Classifier\",\"Linear SVM\"]\n",
    "accuracyComprison(classifiers = listOfClassifiers,accuracies = listOfTestAccuracies, accuraciesNew = listTestAccNew)    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference\n",
    "\n",
    "### References\n",
    "\n",
    "1. https://stats.stackexchange.com/questions/58214/when-does-naive-bayes-perform-better-than-svm\n",
    "\n",
    "2. https://towardsdatascience.com/machine-learning-nlp-text-classification-using-scikit-learn-python-and-nltk-c52b92a7c73a\n",
    "\n",
    "3. Lab-6 https://dal.brightspace.com/d2l/le/content/97462/viewContent/1340766/View\n",
    "\n",
    "4. Lab-3 https://dal.brightspace.com/d2l/le/content/97462/viewContent/1332237/View\n",
    "\n",
    "5. https://stackoverflow.com/questions/35360081/naive-bayes-vs-svm-for-classifying-text-data\n",
    "\n",
    "6. https://stats.stackexchange.com/questions/58214/when-does-naive-bayes-perform-better-than-svm\n",
    "\n",
    "7. https://stats.stackexchange.com/questions/73944/what-are-the-limitations-of-kernel-methods-and-when-to-use-kernel-methods\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
